{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMK/Y/0kMjHgQsqycfgRR6D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UiinKim/UiinKim/blob/main/Word2Vec%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PrTWyLwst16k"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")\n",
        "#urllib을 통해 다른 사람이 만들어 놓은 xml형식의 훈련 데이터를 가져온다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X79tSxSUuC3V",
        "outputId": "669201b0-fd08-44ae-b22f-fe57d2bf7d71"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7f6e2dbacaf0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targetXML=open('ted_en-20160408.xml','r',encoding='UTF8')\n",
        "target_text=etree.parse(targetXML)\n",
        "#xml파일을 여기서 읽을 수 있도록 바꿔줌(etree, parse는 문장에서 주어 목적어 보어 등으로 나누는 역할)\n",
        "\n",
        "parse_text='\\m'.join(target_text.xpath('//content/text()'))\n",
        "#content와 </content> 사이의 내용만 가져온다."
      ],
      "metadata": {
        "id": "VjCf7wQDuhsk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_text=re.sub(r'\\([^)]*\\)','',parse_text)\n",
        "#정규표현식->과로를 모두 제거 제거"
      ],
      "metadata": {
        "id": "7kj3bFTOxiKq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ1uq7qd1E23",
        "outputId": "2c2ed4f2-e9f9-4414-aebf-8415757ab103"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_text=sent_tokenize(content_text)\n",
        "#nltk를 이용하여 입력 말뭉치(corpus)에 대하여 문장 토큰화"
      ],
      "metadata": {
        "id": "FzlinZLwyoHv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_text=[]\n",
        "for string in sent_text:\n",
        "  tokens=re.sub(r\"[^a-z0-9]+\",\" \",string.lower())\n",
        "  #소문자와 숫자는 그대로 쓰고 대문자만 소문자로 변경\n",
        "  normalized_text.append(tokens)\n",
        "  #리스트에 토큰들 추가"
      ],
      "metadata": {
        "id": "_GNRSst81kHd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=[word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "metadata": {
        "id": "dztFMEv32xH-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('총 샘플의 개수 : {}'.format(len(result)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZdjNmC726V9",
        "outputId": "425649f6-e8a3-4bfa-989b-64f2cb0c3a45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플의 개수 : 273170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for line in result[:10]:\n",
        "  print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz1WdqpO3JWl",
        "outputId": "d7fb869b-368c-4956-cc65-6501e3a18f38"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
            "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
            "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n",
            "['consider', 'facit']\n",
            "['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them']\n",
            "['facit', 'was', 'a', 'fantastic', 'company']\n",
            "['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world']\n",
            "['everybody', 'used', 'them']\n",
            "['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along']\n",
            "['they', 'continued', 'doing', 'exactly', 'the', 'same']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import keyedvectors\n",
        "\n",
        "model=Word2Vec(sentences=result, vector_size=100,window=5,min_count=5,workers=4,sg=0)\n",
        "\n",
        "#하이퍼 파라미터 sentences= 말뭉치를 기준으로 분리된 형태소들로 이루어진 문장들(리스트)\n",
        "#vector_size=임베딩 된 벡터의 차원\n",
        "#window=해당 단어의 앞과 뒤의 단어들의 수\n",
        "#min_count=문장들을 모아놓은 리스트에서 출현한 단어의 최소 횟수(min_count 이하는 세지 않음),\n",
        "#workers=동시에 처리할 프로세스(작업)의 수-> 코어의 수와 비슷하게 설정한다.\n",
        "#sg=0이면 CBOW, sg=1이면 Skip-gram\n",
        "#CBOW는 (continuous bag of words) 주변 단어의 임베딩 --> 대상 단어 예측\n",
        "#Skip-gram은 대상 단어의 임베딩 --> 주변 단어 예측"
      ],
      "metadata": {
        "id": "Bn2zt_4f3Y9k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_result=model.wv.most_similar(\"man\")\n",
        "#(단어)와 유사한 단어들을 model에서 예측하여 출력\n",
        "print(model_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjTeHcCx4ob9",
        "outputId": "ae5f41cc-c703-41e2-c1f5-3bea0c47f735"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.8526089787483215), ('guy', 0.8194919228553772), ('lady', 0.7678653001785278), ('girl', 0.7389764189720154), ('boy', 0.7364939451217651), ('soldier', 0.7299680113792419), ('gentleman', 0.7264711856842041), ('poet', 0.6935451626777649), ('david', 0.6789766550064087), ('kid', 0.666795015335083)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.word2vec import KeyedVectors\n",
        "model.wv.save_word2vec_format('eng_w2v')\n",
        "#모델 저장\n",
        "loaded_model=KeyedVectors.load_word2vec_format(\"eng_w2v\")\n",
        "#모델 로드"
      ],
      "metadata": {
        "id": "5vbskeaXivV3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medel_result=loaded_model.most_similar(\"man\")\n",
        "print(model_result)\n",
        "#이미 저장되어서 어떤 단어가 들어가던지 이미 들어가진 단어로 로드됨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps_Wf79Lk4Wg",
        "outputId": "e9d93667-0465-48bd-dda8-97c37b5cf663"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.8526089787483215), ('guy', 0.8194919228553772), ('lady', 0.7678653001785278), ('girl', 0.7389764189720154), ('boy', 0.7364939451217651), ('soldier', 0.7299680113792419), ('gentleman', 0.7264711856842041), ('poet', 0.6935451626777649), ('david', 0.6789766550064087), ('kid', 0.666795015335083)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGsvOnDtl1y-",
        "outputId": "30d5c17e-3d37-46c1-bbcf-0298a2e32599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "JAPdS4DDlG9t",
        "outputId": "28ec2219-ea77-4584-96fa-618115aea4bf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-cd7343e146a8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
      ],
      "metadata": {
        "id": "C4Dc7PkTmG_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VquAuuh5uit-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}