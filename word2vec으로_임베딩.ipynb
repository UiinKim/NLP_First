{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLpVcnErEIbG5C/E5lrAVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UiinKim/UiinKim/blob/main/word2vec%EC%9C%BC%EB%A1%9C_%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "VHQRWdG6OTvz"
      },
      "outputs": [],
      "source": [
        "#Word2Vec은 Skip-gram과 CBOW이 있는데 Skip-gram은 한 단어로 주변 단어들을 예측 하는 것이고, Word2Vec은 주변 단어로 한 단어를 예측하는 것이다.\n",
        "#Skip-gram이 성능이 더 좋지만 더 많은 단어를 예측해야 하기 때문에 비효율적인 부분이 있다.\n",
        "#그래서 현재는 negative sampling(두 단어를 입력으로 넣어 한 윈도우에 등장한 적이 있으면 1, 없으면 0)으로 SGNS(Skip-gram with negative sampling)을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "res=requests.get('http://www.gutenberg.org/files/2591/2591-0.txt')\n",
        "#requests.get(), post(), put(), delete() ->HTTP요청 방식\n",
        "#res.status_code를 해보면 형태를 알 수 있음"
      ],
      "metadata": {
        "id": "TUnYJqOJPs3J"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grimm=res.text[2801:530661]\n",
        "#res.content : 바이너리 원문\n",
        "#res.text : UTF-8로 인코딩 된 문자열\n",
        "#res.json() : dictionary객체"
      ],
      "metadata": {
        "id": "T6fDjc42P5A_"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grimm=re.sub(r'[^a-zA-Z\\.]', ' ', grimm) #알파벳만 추출, 불필요한 단어는 제외\n",
        "sentences=grimm.split('. ') #문장 단위로 자르기"
      ],
      "metadata": {
        "id": "VzU8CCB9Q7eJ"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le6RuXQhRTb3",
        "outputId": "502e3264-1f1d-4463-ec61-a71590260603"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  SECOND STORY       THE SALAD       THE STORY OF THE YOUTH WHO WENT FORTH TO LEARN WHAT FEAR WAS       KING GRISLY BEARD       IRON HANS       CAT SKIN       SNOW WHITE AND ROSE RED          THE BROTHERS GRIMM FAIRY TALES          THE GOLDEN BIRD      A certain king had a beautiful garden  and in the garden stood a tree  which bore golden apples',\n",
              " 'These apples were always counted  and about  the time when they began to grow ripe it was found that every night one  of them was gone',\n",
              " 'The king became very angry at this  and ordered the  gardener to keep watch all night under the tree',\n",
              " 'The gardener set his  eldest son to watch  but about twelve o   clock he fell asleep  and in  the morning another of the apples was missing',\n",
              " 'Then the second son was  ordered to watch  and at midnight he too fell asleep  and in the morning  another apple was gone']"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 단위로 자르기\n",
        "data=[s.split() for s in sentences]\n",
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JItZuOjsRWZP",
        "outputId": "7e755e81-ad87-4ef7-d51a-a0a517cf0852"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SECOND',\n",
              " 'STORY',\n",
              " 'THE',\n",
              " 'SALAD',\n",
              " 'THE',\n",
              " 'STORY',\n",
              " 'OF',\n",
              " 'THE',\n",
              " 'YOUTH',\n",
              " 'WHO',\n",
              " 'WENT',\n",
              " 'FORTH',\n",
              " 'TO',\n",
              " 'LEARN',\n",
              " 'WHAT',\n",
              " 'FEAR',\n",
              " 'WAS',\n",
              " 'KING',\n",
              " 'GRISLY',\n",
              " 'BEARD',\n",
              " 'IRON',\n",
              " 'HANS',\n",
              " 'CAT',\n",
              " 'SKIN',\n",
              " 'SNOW',\n",
              " 'WHITE',\n",
              " 'AND',\n",
              " 'ROSE',\n",
              " 'RED',\n",
              " 'THE',\n",
              " 'BROTHERS',\n",
              " 'GRIMM',\n",
              " 'FAIRY',\n",
              " 'TALES',\n",
              " 'THE',\n",
              " 'GOLDEN',\n",
              " 'BIRD',\n",
              " 'A',\n",
              " 'certain',\n",
              " 'king',\n",
              " 'had',\n",
              " 'a',\n",
              " 'beautiful',\n",
              " 'garden',\n",
              " 'and',\n",
              " 'in',\n",
              " 'the',\n",
              " 'garden',\n",
              " 'stood',\n",
              " 'a',\n",
              " 'tree',\n",
              " 'which',\n",
              " 'bore',\n",
              " 'golden',\n",
              " 'apples']"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHXDN1YGRoO_",
        "outputId": "f85377a1-f37f-4708-ee18-7d6983f831d9"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.word2vec import Word2Vec"
      ],
      "metadata": {
        "id": "zdFf2010RtpQ"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Word2Vec(data, sg=1, vector_size=100, window=3, min_count=3, workers=4)"
      ],
      "metadata": {
        "id": "VFypDbrYSE5H"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('word2vec.model')\n",
        "model=Word2Vec.load('word2vec.model')"
      ],
      "metadata": {
        "id": "2UKFT_SFSQgI"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv['princess'] #모델을 벡터로 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lrATmMcStl4",
        "outputId": "f27127ac-472a-40e1-e71b-c46f8ec8e09a"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.22418244,  0.18808486, -0.06708387,  0.18774956, -0.02416245,\n",
              "       -0.3169132 ,  0.1241987 ,  0.52533126, -0.25269324, -0.20108257,\n",
              "        0.07941759, -0.2594471 , -0.00074198,  0.11370037,  0.05706058,\n",
              "       -0.27027124,  0.11912285, -0.15775053, -0.14800896, -0.1958774 ,\n",
              "        0.2878406 ,  0.19286273,  0.30040908, -0.10773802,  0.05694722,\n",
              "        0.0009996 , -0.02515211, -0.09477367, -0.07858583, -0.05967469,\n",
              "        0.06484113, -0.09005069,  0.17382313, -0.28651625, -0.1671547 ,\n",
              "        0.18184179,  0.02222012, -0.0377952 , -0.05486879, -0.1164223 ,\n",
              "       -0.03078176, -0.07752863, -0.01995263, -0.01113612,  0.00805428,\n",
              "        0.07329975, -0.09491224, -0.08919481,  0.23763569, -0.01243767,\n",
              "        0.11772417, -0.05253872, -0.09553364, -0.16688766,  0.18591496,\n",
              "        0.08235055,  0.16753834,  0.00711078, -0.17106007,  0.12355166,\n",
              "       -0.03546011, -0.00554178,  0.16267815, -0.1770557 , -0.24663684,\n",
              "        0.10107991, -0.00252757,  0.32514557, -0.18702058,  0.23060045,\n",
              "       -0.06256971,  0.0120589 ,  0.1576034 , -0.06425097,  0.30631375,\n",
              "       -0.1227861 ,  0.03074636, -0.02713836, -0.1059897 ,  0.03796459,\n",
              "       -0.25637376,  0.0238966 , -0.20455693,  0.20779368, -0.13537803,\n",
              "       -0.07758586,  0.04591818,  0.17023736,  0.08292044, -0.00458064,\n",
              "        0.0894101 ,  0.04986412, -0.05841888,  0.09740727,  0.16836128,\n",
              "        0.16451441,  0.05486725, -0.23636211, -0.15996808, -0.17334677],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.similarity('princess', 'queen') #유사도 검사=유비"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaFe3TIySvhw",
        "outputId": "7b761137-89d6-4d92-95cc-cac2d294c4c3"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9733399"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('princess')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_7_wGjVS5l5",
        "outputId": "6882d488-05eb-406e-84d8-7ec03dd40df6"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('prince', 0.987869381904602),\n",
              " ('boy', 0.9800905585289001),\n",
              " ('youth', 0.9797530174255371),\n",
              " ('dwarf', 0.9781355261802673),\n",
              " ('soldier', 0.9771094918251038),\n",
              " ('third', 0.9762648344039917),\n",
              " ('youngest', 0.9760798215866089),\n",
              " ('wedding', 0.9746068716049194),\n",
              " ('cook', 0.9740585684776306),\n",
              " ('palace', 0.9739507436752319)]"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('prince')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_hwrBrd-elP",
        "outputId": "33bf48b8-aa86-4fdd-bb2e-4007530c03f6"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cook', 0.988935649394989),\n",
              " ('princess', 0.987869381904602),\n",
              " ('youth', 0.987262487411499),\n",
              " ('huntsmen', 0.9850553870201111),\n",
              " ('youngest', 0.9841073155403137),\n",
              " ('fisherman', 0.9832018613815308),\n",
              " ('eldest', 0.9824524521827698),\n",
              " ('soldier', 0.9814544320106506),\n",
              " ('wedding', 0.9813396334648132),\n",
              " ('dwarf', 0.9810577630996704)]"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#woman:princess = man:?의 유비를 풀어보자\n",
        "model.wv.most_similar(positive=['man','princess'], negative=['woman'])\n",
        "#man vector + princess voctor - woman vector로 벡터로 생각을 해본다. 코드로 꼭 뜯어서 다시 봐보기.\n",
        "#벡터의 덧셈은 a에서 b를 이어서 생각하기, 뺄셈은 b에서 a로 가기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QeHzzINS_5A",
        "outputId": "e9dc58bd-f51b-4507-e456-5fc0a40303b1"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bird', 0.9639790654182434),\n",
              " ('cat', 0.9526981115341187),\n",
              " ('cow', 0.950363039970398),\n",
              " ('peasant', 0.9391450881958008),\n",
              " ('giant', 0.9382414221763611),\n",
              " ('dog', 0.9373616576194763),\n",
              " ('mouse', 0.9365424513816833),\n",
              " ('shepherd', 0.9341973066329956),\n",
              " ('bride', 0.9335201978683472),\n",
              " ('gardener', 0.9334748983383179)]"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#keras Embedding 사용법\n",
        "#문장 토큰화(Tokenizer) -> 토큰화 된 단어를 정수로 변환(texts_to_sequences) -> 문장 길이를 모두 동일하게 패딩(pad_sequences) -> 임베딩 층에 주입(model.add, Embedding)\n",
        "#gensim으로 학습된 단어 임베딩을 keras에서 불러오기\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "NUM_WORDS, EMB_DIM=model.wv.vectors.shape #gensim으로 학습된 단어 임베딩의 매트릭스의 크기를 저장한다."
      ],
      "metadata": {
        "id": "T2TZ1mFDTkgg"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#전이 학습 : 한 가지 문제에 대해 학습한 기능을 가져와서 새로운 문제에 활용하는 것. 일반적으로 전체 모델을 처음부터 훈련하기에는 데이터세트에 데이터가 너무 적을 때 수행.\n",
        "emb=Embedding(input_dim=NUM_WORDS, output_dim=EMB_DIM, trainable=False, weights=[model.wv.vectors])\n",
        "#input_dims=단어 사전(input layer)의 크기, output_dims= 출력의 차원, trainable=False이면 모든 레이어의 가중치가 훈련 불가능으로 이동 --> '동결', weights= 가중치 변수 목록"
      ],
      "metadata": {
        "id": "g3pK7lPXYTDc"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net=Sequential() #net에 새로운 모델 세팅\n",
        "net.add(emb)## net에 첫번째 레이어 생성"
      ],
      "metadata": {
        "id": "jcFTuA8GaUnq"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "eSxEGfVwn-Hm"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=model.wv.index_to_key.index('princess') #princess의 임베딩된 벡터값이 gensim과 같다는 것을 확인 가능\n",
        "net.predict([i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K82Eved5mf5W",
        "outputId": "f567a817-8dd8-4fe8-f1c0-0cdac3869453"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 235ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.22418244,  0.18808486, -0.06708387,  0.18774956, -0.02416245,\n",
              "        -0.3169132 ,  0.1241987 ,  0.52533126, -0.25269324, -0.20108257,\n",
              "         0.07941759, -0.2594471 , -0.00074198,  0.11370037,  0.05706058,\n",
              "        -0.27027124,  0.11912285, -0.15775053, -0.14800896, -0.1958774 ,\n",
              "         0.2878406 ,  0.19286273,  0.30040908, -0.10773802,  0.05694722,\n",
              "         0.0009996 , -0.02515211, -0.09477367, -0.07858583, -0.05967469,\n",
              "         0.06484113, -0.09005069,  0.17382313, -0.28651625, -0.1671547 ,\n",
              "         0.18184179,  0.02222012, -0.0377952 , -0.05486879, -0.1164223 ,\n",
              "        -0.03078176, -0.07752863, -0.01995263, -0.01113612,  0.00805428,\n",
              "         0.07329975, -0.09491224, -0.08919481,  0.23763569, -0.01243767,\n",
              "         0.11772417, -0.05253872, -0.09553364, -0.16688766,  0.18591496,\n",
              "         0.08235055,  0.16753834,  0.00711078, -0.17106007,  0.12355166,\n",
              "        -0.03546011, -0.00554178,  0.16267815, -0.1770557 , -0.24663684,\n",
              "         0.10107991, -0.00252757,  0.32514557, -0.18702058,  0.23060045,\n",
              "        -0.06256971,  0.0120589 ,  0.1576034 , -0.06425097,  0.30631375,\n",
              "        -0.1227861 ,  0.03074636, -0.02713836, -0.1059897 ,  0.03796459,\n",
              "        -0.25637376,  0.0238966 , -0.20455693,  0.20779368, -0.13537803,\n",
              "        -0.07758586,  0.04591818,  0.17023736,  0.08292044, -0.00458064,\n",
              "         0.0894101 ,  0.04986412, -0.05841888,  0.09740727,  0.16836128,\n",
              "         0.16451441,  0.05486725, -0.23636211, -0.15996808, -0.17334677]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "#미리 학습시킨 Word2Vec 임베딩을 다운 받을 수 있다."
      ],
      "metadata": {
        "id": "67X87Kr0nJTm"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "#영화, 배우, 텔레비전 드라마, 비디오게임 등에 관한 정보를 제공하는 온라인 데이터베이스"
      ],
      "metadata": {
        "id": "6Bn8WehBFwyf"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 데이터 준비 단계\n",
        " (x_train, y_train), (x_test, y_test)=imdb.load_data()\n",
        " #데이터 셋을 불러온다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JSWvZ59F1Ev",
        "outputId": "9b4cc9b2-6ea6-4712-904d-41886348c213"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index=imdb.get_word_index()\n",
        "#단어의  index를 불러온다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxhJoVZKGGtI",
        "outputId": "8f686be0-2389-4463-a3b1-d0da64c680f7"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_word={idx+3:word for word, idx in word_index.items()}\n",
        "#단어 번호와 단어를 dictionary로 만든다. ex) {615 : 'seriously'}\n",
        "index_word[1]='<START>'\n",
        "index_word[2]='<UNKNOWN>'\n",
        "#1번은 문장의 시작, 2번은 사전에 없는 단어로 미리 지정"
      ],
      "metadata": {
        "id": "xIlj5LqgGOEy"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(index_word[i] for i in x_train[0])\n",
        "#.join은 한 데이트베이스 내의 여러 테이블의 레코드를 조합하여 하나의 열로 표현"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "0oIkAW_wGUTv",
        "outputId": "41aff04a-98ff-46c9-aefd-2d425e313178"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_WORDS=max(index_word)+1\n",
        "#총 단어의 개수"
      ],
      "metadata": {
        "id": "ItWlvcy9HV_I"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 텍스트를 단어 번호로 바꾸기\n",
        "texts=[]\n",
        "for data in x_train:\n",
        "  text=' '.join(index_word[i] for i in data)\n",
        "  texts.append(text)"
      ],
      "metadata": {
        "id": "21weBdIRHtbQ"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqZXstrsI4U2",
        "outputId": "b88e007b-6391-485a-e4c0-4938de57e6a0"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\",\n",
              " \"<START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal measures the hair is big lots of boobs bounce men wear those cut tee shirts that show off their stomachs sickening that men actually wore them and the music is just synthesiser trash that plays over and over again in almost every scene there is trashy music boobs and paramedics taking away bodies and the gym still doesn't close for bereavement all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\",\n",
              " \"<START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had earnt working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how embarrasing this is to watch save yourself an hour a bit of your life\",\n",
              " \"<START> the scots excel at storytelling the traditional sort many years after the event i can still see in my mind's eye an elderly lady my friend's mother retelling the battle of culloden she makes the characters come alive her passion is that of an eye witness one to the events on the sodden heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and breadth of scotland as i discussed it with a friend one night in mallaig a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional reservoir fact and fiction blend with aspiration role models warning stories archetypes magic and mystery br br my name is aonghas like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the impenetrable wonder of scotland its rugged mountains shrouded in mists the stuff of legend yet seach'd is rooted in reality this is what gives it its special charm it has a rough beauty and authenticity tempered with some of the finest gaelic singing you will ever hear br br aonghas angus visits his grandfather in hospital shortly before his death he burns with frustration part of him yearns to be in the twenty first century to hang out in glasgow but he is raised on the western shores among a gaelic speaking community br br yet there is a deeper conflict within him he yearns to know the truth the truth behind his grandfather's ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last fateful journey to the summit of one of scotland's most inaccessible mountains can the truth be told or is it all in stories br br in this story about stories we revisit bloody battles poisoned lovers the folklore of old and the sometimes more treacherous folklore of accepted truth in doing so we each connect with angus as he lives the story of his own life br br seachd the inaccessible pinnacle is probably the most honest unpretentious and genuinely beautiful film of scotland ever made like angus i got slightly annoyed with the pretext of hanging stories on more stories but also like angus i forgave this once i saw the 'bigger picture ' forget the box office pastiche of braveheart and its like you might even forego the justly famous dramatisation of the wicker man to see a film that is true to scotland this one is probably unique if you maybe meditate on it deeply enough you might even re evaluate the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced\",\n",
              " \"<START> worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the microwave and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life\"]"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)\n",
        "#x_train에 있는 data들의 단어들을 모두 texts에 저장시킨다.(문자열 상태)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjM2kEEeICoX",
        "outputId": "140dfb0a-a091-4bbb-8e53-ce362dd8423f"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tok=Tokenizer()\n",
        "tok.fit_on_texts(texts) #texts에 있는 단어들에 번호를 매긴다\n",
        "new_data=tok.texts_to_sequences(texts) #텍스트를 실제로 단어 번호 리스트로 변환\n",
        "new_data[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POAvP7ryIQTo",
        "outputId": "98f1ef1f-298e-4805-a861-2d044b2fbe53"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[28, 11, 19, 13, 41, 526, 968, 1618, 1381, 63]"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGLiOLV4Kgog",
        "outputId": "bae88c44-6f67-440b-be91-1fb4a0c7cfba"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.단어쌍 만들기\n",
        "from keras.preprocessing.sequence import make_sampling_table, skipgrams\n",
        "VOCAB_SIZE=len(tok.word_index)"
      ],
      "metadata": {
        "id": "EPPtE7X9Kpbp"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#단어를 무작위로 추출하면 자주 나오는 단어가 자주 추출되기 때문에 이를 방지하기 위해 균형을 맞춘 샘플링 표를 만든다\n",
        "table=make_sampling_table(VOCAB_SIZE)\n",
        "#두 단어씩 뽑아 좌우 2단어 안에 들어있는 경우가 있는지 없는지를 확인한다.\n",
        "couples, labels=skipgrams(data, VOCAB_SIZE, window_size=2, sampling_table=table)"
      ],
      "metadata": {
        "id": "XBNErngqK-HP"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "couples[:5]\n",
        "#대상 단어와 맥락 단어가 세트로 들어가있다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w4wVgoTL5-q",
        "outputId": "f0f47a55-9953-47f1-cff0-309d64dae32c"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[115, 47661], [1310, 4], [70907, 76064], [2642, 272], [115, 50]]"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[:5]\n",
        "#겹치는지 안 겹치는지 1과 0으로 나누어져있다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxerCb-HL-s5",
        "outputId": "34bc7aa7-abbc-4a3a-fbdd-cfd85ea7371a"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_target, word_context=zip(*couples) #2차원 리스트라서 *을 붙혀서 column끼리 대상 단어와 맥락 단어를 각각 word_target과 word_context로 넣어준다"
      ],
      "metadata": {
        "id": "RimQpmyzMBIp"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "2MqYxQTtMRwx"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_target=np.array(word_target, dtype='int32')\n",
        "word_context=np.array(word_context, dtype='int32')"
      ],
      "metadata": {
        "id": "-ufROebsM8ZC"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, Dot, Embedding, Flatten, Input, Reshape\n",
        "from keras.models import Model\n",
        "#skip-gram모형은 sequential 대신 각 레이어를 함수처럼 사용하는 Functional API를 사용해야 한다."
      ],
      "metadata": {
        "id": "ODdqtkuMNDoS"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_target=Input(shape=(1,))\n",
        "input_context=Input(shape=(1,))\n",
        "#입력 레이어를 만든다."
      ],
      "metadata": {
        "id": "r1vWj1RPNOLa"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb=Embedding(input_dim=VOCAB_SIZE, output_dim=89)\n",
        "#임베딩 레이어를 만든다."
      ],
      "metadata": {
        "id": "-8psLPheOOoK"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target=emb(input_target)\n",
        "context=emb(input_context)\n",
        "#대상 단어를 입력으로 받는 임베딩 레이어 target\n",
        "#맥락 단어를 입력으로 받는 임베딩 레이어 context"
      ],
      "metadata": {
        "id": "1rlVHkWXOaJr"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#두 임베딩의 내적을 구한다. -> 두 벡터가 비슷할수록 값은 커진다.\n",
        "dot=Dot(axes=2)([target, context])"
      ],
      "metadata": {
        "id": "o51rPHqsOktC"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat=Reshape((1,))(dot)"
      ],
      "metadata": {
        "id": "RImakZWeQop7"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out=Activation('sigmoid')(flat)"
      ],
      "metadata": {
        "id": "0sF8FjkRQq-K"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram=Model(inputs=[input_target, input_context], outputs=out)"
      ],
      "metadata": {
        "id": "IUc5s85IQuhT"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpuoTpHWQ8dc",
        "outputId": "3e334ce8-25a6-452f-fcda-02d33a5263a8"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 1, 89)        7883709     ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 1, 1)         0           ['embedding_4[0][0]',            \n",
            "                                                                  'embedding_4[1][0]']            \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 1)            0           ['dot[0][0]']                    \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1)            0           ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,883,709\n",
            "Trainable params: 7,883,709\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i-h5F2d-RAFi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}