{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UiinKim/NLP_First/blob/main/2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO3MdMZNbjg2"
      },
      "source": [
        "1.1.1 벡터와 행렬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vxgWiu7bjg4",
        "outputId": "f21f89f6-f732-4133-dc39-3a6f51c6888c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#모든 벡터는 행벡터로 다룬다\n",
        "#vector x, matrix W\n",
        "import numpy as np\n",
        "\n",
        "x=np.array([1, 2, 3])\n",
        "x.__class__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY7TcWLQbjg6",
        "outputId": "f1d2eb4e-33f1-4d6a-b78a-f074d7d79823"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anuPbgIRbjg6",
        "outputId": "631df111-a443-4810-ad81-03f50e8851fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra4EhGDYbjg7",
        "outputId": "33a404e8-0f7d-431f-d024-7a8fdfb75444"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W=np.array([[1, 2, 3], [4, 5, 6]])\n",
        "W.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpMIUlaAbjg7",
        "outputId": "2fb8597d-9b30-4549-e36a-7fd636981119"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS4nLaw9bjg7"
      },
      "outputs": [],
      "source": [
        "#vector, matrix는 np.array()로 생성\n",
        "#shape은 다차원 배열의 형상 -> vector : 원소의 수가 3개, matrix : 2차원 배열로 각 차원별로 3개의 원소 총 6개의 원소가 존재\n",
        "#ndim은 차원의 수 -> vector : 1차원, matrix : 2차원"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjzMDnRobjg8"
      },
      "source": [
        "1.1.2 행렬의 원소별 연산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV0VKanvbjg8",
        "outputId": "f209c94d-2afb-4323-c5c6-0a6c16171556"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1,  3,  5],\n",
              "       [ 7,  9, 11]])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W=np.array([[1, 2, 3], [4, 5, 6]])\n",
        "X=np.array([[0, 1, 2], [3, 4, 5]])\n",
        "W+X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-QJHLXVbjg8",
        "outputId": "e8bf3c4f-26ce-4047-9b45-e26add921f44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0,  2,  6],\n",
              "       [12, 20, 30]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W*X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXffgZelbjg9"
      },
      "source": [
        "1.1.3 브로드캐스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHjwcKhqbjg9",
        "outputId": "84004e0e-d3fb-460a-9451-a5dc439a145b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[10, 20],\n",
              "       [30, 40]])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A=np.array([[1, 2], [3, 4]])\n",
        "A*10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmaH9op8bjg9"
      },
      "outputs": [],
      "source": [
        "#2x2 행렬에 스칼라 값인 10을 곱하여 스칼라 값 10이 2x2행렬로 확장된 후 원소별 연산 수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuKK8vJ6bjg-",
        "outputId": "12b72ec2-ffe3-4027-999a-9792e4fa8412"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[10, 40],\n",
              "       [30, 80]])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A=np.array([[1, 2], [3, 4]])\n",
        "b=np.array([10, 20])\n",
        "A*b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFepQaPcbjg-"
      },
      "source": [
        "1.1.4 벡터의 내적과 행렬의 곱"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdI8dh1dbjg-"
      },
      "outputs": [],
      "source": [
        "a=np.array([1, 2, 3])\n",
        "b=np.array([4, 5, 6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FONRNQafbjg-",
        "outputId": "0e14fb48-5ef1-44ed-91c3-93377e39cccf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.dot(a, b)\n",
        "#x내적y는 x와 y의 원소들의 모든 곱의 합"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqJ9jwGabjg_"
      },
      "outputs": [],
      "source": [
        "A=np.array([[1, 2], [3, 4]])\n",
        "B=np.array([[5, 6], [7, 8]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWxB1mhMbjg_",
        "outputId": "e83e39ee-6a3c-4ad6-b9ca-b7312e97dfc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[19, 22],\n",
              "       [43, 50]])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#행렬곱은 각 행과 열에 맞는 값들을 곱하여 계산 (1x5 + 2x7 = 19)\n",
        "np.matmul(A, B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixWF7gw7bjg_"
      },
      "source": [
        "1.2.1 신경망"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NL8h5PpbjhA"
      },
      "outputs": [],
      "source": [
        "W1=np.random.randn(2, 4)\n",
        "b1=np.random.randn(4)\n",
        "x=np.random.randn(10, 2)\n",
        "h=np.matmul(x, W1)+b1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TReMjztbjhA"
      },
      "outputs": [],
      "source": [
        "#FC는 선형 변환\n",
        "#비선형 효과를 부여 : 활성화 함수 -> sigmoid(S자)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eSX5IUebjhA"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMEuN7j9bjhA"
      },
      "outputs": [],
      "source": [
        "a=sigmoid(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_RzA4_XbjhA"
      },
      "outputs": [],
      "source": [
        "x=np.random.randn(10, 2) #2차원의 데이터 10개가 미니배치로 처리\n",
        "W1=np.random.randn(2, 4)\n",
        "b1=np.random.randn(4)\n",
        "W2=np.random.randn(4, 3)\n",
        "b2=np.random.randn(3)\n",
        "\n",
        "h=np.matmul(x, W1) + b1\n",
        "a=sigmoid(h)\n",
        "s=np.matmul(a, W2) + b2 #(10, 3) shape\n",
        "#s는 3차원으로 출력이 되며 3가지의 분류 케이스 점수(확률이 되기 전 score) -> softmax를 거쳐서 확률이 됨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrvi05F1bjhB"
      },
      "source": [
        "1.2.2 계층으로 클래스화 및 순전파 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYR5_mObbjhB"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.params=[] #가중치와 편향 같은 parameters 담는 리스트\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU5E1oy0bjhB"
      },
      "outputs": [],
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params=[W, b]\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.params #신경망이 학습될 때 수시로 갱신\n",
        "        out = np.matmul(x, W) + b\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bPCOHYAbjhB"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNet:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        I, H, O = input_size, hidden_size, output_size\n",
        "\n",
        "        #가중치 및 편향 초기화\n",
        "        W1=np.random.randn(I, H)\n",
        "        b1=np.random.randn(H)\n",
        "        W2=np.random.randn(H, O)\n",
        "        b2=np.random.randn(O)\n",
        "\n",
        "        #계층 생성\n",
        "        self.layers=[\n",
        "            Affine(W1, b1),\n",
        "            Sigmoid(),\n",
        "            Affine(W2, b2)\n",
        "        ]\n",
        "\n",
        "        #모든 가중치를 리스트에 모은다.\n",
        "        self.params=[]\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_nfgKDLbjhB",
        "outputId": "67abda0d-e4c7-4211-a5ce-741326ac943c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.09611793  0.9182776  -0.1870312 ]\n",
            " [-0.65746549  0.7134409  -0.92772757]\n",
            " [-0.55644143  0.83206906 -0.93542695]\n",
            " [-0.28308649  0.76701186 -0.35761376]\n",
            " [-0.45980443  0.86420867 -0.77239211]\n",
            " [-0.53766125  0.81183541 -0.849522  ]\n",
            " [-0.23998178  0.72896561 -0.21746912]\n",
            " [-0.48715621  0.85331573 -0.81224785]\n",
            " [-0.48094816  0.86869192 -0.82724536]\n",
            " [-0.49308095  0.83313678 -0.79336479]]\n"
          ]
        }
      ],
      "source": [
        "#예측해보기\n",
        "x=np.random.randn(10, 2)\n",
        "model=TwoLayerNet(2, 4, 3)\n",
        "s=model.predict(x)\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbNq_V__bjhC"
      },
      "source": [
        "1.3 노드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3u7XzAvbjhC"
      },
      "outputs": [],
      "source": [
        "D, N = 8, 7\n",
        "x=np.random.randn(1, D) #input\n",
        "y=np.repeat(x, N, axis=0) #forward\n",
        "dy=np.random.randn(N, D) #무작위 기울기\n",
        "dx=np.sum(dy, axis=0, keepdims=True) #backward, 차원 유지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38GzqaoibjhC"
      },
      "outputs": [],
      "source": [
        "D, N=8, 7\n",
        "x=np.random.randn(N, D) #input\n",
        "y=np.sum(x, axis=0, keepdims=True) #forward -> 하나로 합쳐짐\n",
        "\n",
        "dy=np.random.randn(1, D) #random 기울기\n",
        "dx=np.repeat(dy, N, axis=0) # backward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxWVTuzlbjhC"
      },
      "outputs": [],
      "source": [
        "class MatMul:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)] #grads에 기울기 W보관하기 위해 리스트 선언\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, = self.params\n",
        "        out = np.matmul(x, W)\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, = self.params\n",
        "        dx = np.matmul(dout, W.T) #x이므로 W.T와 dout\n",
        "        dW = np.matmul(self.x.T, dout) #W이므로 x.T와 dout\n",
        "        self.grads[0][...] = dW #역전파가 된 기울기를 grads에 보관\n",
        "        #생략기호 ...을 활용하여 덮어쓰기 수행 -> 메모리 주소 고정된 자리에 복사(리스트이기 때문에 중요)\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdBO9oE0bjhD"
      },
      "outputs": [],
      "source": [
        "a=np.array([1, 2, 3])\n",
        "b=np.array([4,5,6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGYZDVUKbjhD"
      },
      "outputs": [],
      "source": [
        "#a=b를 하면 a가 b로 메모리 위치가 같아지는 것이고\n",
        "#a[...]=b를 하면 메모리 위치가 아닌 내용 4, 5, 6만 복사하여 a의 해당된 메모리에 저장하는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_81wxrubjhD"
      },
      "source": [
        "1.3.5 기울기 도출과 역전파 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0we6NnR4bjhD"
      },
      "outputs": [],
      "source": [
        "#sigmoid의 식을 미분하면 y(1-y) -> y:output\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out=None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1+np.exp(-x))\n",
        "        self.out=out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx=dout * (1.0-self.out)*self.out\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiIi_OlZbjhD"
      },
      "outputs": [],
      "source": [
        "#Affine 층은 fully connected 층으로 x*W에 b를 더하는데 b는 은닉층과 x의 수 N에 맞춰 결정됨\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params=[W, b]\n",
        "        self.grads=[np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x=None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b=self.params\n",
        "        out=np.matmul(x, W)+b #b는 브로드캐스팅 되어서 자동으로 N개 적용(repeat)\n",
        "        self.x=x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, b=self.params\n",
        "        dx=np.matmul(dout, W.T)\n",
        "        dW=np.matmul(self.x.T, dout)\n",
        "        db=np.sum(dout, axis=0)\n",
        "\n",
        "        self.grads[0][...]=dW\n",
        "        self.grads[1][...]=db\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1xsAqcZbjhF"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr=lr\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        for i in range(len(params)):\n",
        "\n",
        "            params[i]-=self.lr*grads[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lrk8ndMbjhF"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim==2:\n",
        "        x=x-x.max(axis=1, keepdims=True)\n",
        "        x=np.exp(x)\n",
        "        x/=x.sum(axis=1, keepdims=True)\n",
        "\n",
        "    elif x.ndim==1:\n",
        "        x=x-np.max(x)\n",
        "        x=np.exp(x)/np.sum(np.exp(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim==1:\n",
        "        t=t.reshape(1, t.size)\n",
        "        y=y.reshape(1, y.size)\n",
        "\n",
        "    #정답데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
        "    if t.size==y.size:\n",
        "        t=t.argmax(axis=1)\n",
        "\n",
        "    batch_size=y.shape[0]\n",
        "\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t]+1e-7))/batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEEdzj46bjhF"
      },
      "outputs": [],
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        params, grads=[], []\n",
        "        self.out=None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out=softmax(x)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx=self.out*dout #역전파로 구한 순전파의 출력에 대한 기울기를 곱하여 dx를 구한다\n",
        "        sumdx=np.sum(dx, axis=1, keepdims=True) #axis=1로 설정하여 각 dx값마다의 합을 구한다\n",
        "        dx-=self.out*sumdx\n",
        "        return dx\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads=[], []\n",
        "        self.y=None #softmax의 출력\n",
        "        self.t=None #정답 레이블\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t=t\n",
        "        self.y=softmax(x)\n",
        "\n",
        "        #정답 레이블이 원핫베터면 그대로 정답 인덱스로 변환\n",
        "        if self.t.size==self.y.size:\n",
        "            self.t=self.t.argmax(axis=1) #가장 확률이 높은 인덱스 반환\n",
        "\n",
        "        loss=cross_entropy_error(self.y, self.t)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size=self.t.shape[0] #정답 레이블의 batch_size 복사\n",
        "\n",
        "        dx=self.y.copy()\n",
        "        dx[np.arange(batch_size), self.t] -= 1\n",
        "        dx*=dout\n",
        "        dx=dx/batch_size\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym2QcV90bjhF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0otcu8oWbjhG"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNet:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        I, H, O=input_size, hidden_size, output_size\n",
        "\n",
        "        #가중치, 편향 초기화\n",
        "        W1=0.01*np.random.randn(I, H) #가중치를 작은 무작위 값으로 초기화\n",
        "        b1=np.zeros(H)\n",
        "        W2=0.01*np.random.randn(H, O)\n",
        "        b2=np.zeros(O)\n",
        "\n",
        "        #계층 생성\n",
        "        self.layers=[\n",
        "            Affine(W1, b1),\n",
        "            Sigmoid(),\n",
        "            Affine(W2, b2)\n",
        "        ]\n",
        "        self.loss_layer=SoftmaxWithLoss()\n",
        "\n",
        "        #모든 가중치와 기울기를 리스트에 모은다.\n",
        "        self.params, self.grads=[],[]\n",
        "        for layer in self.layers:\n",
        "            self.params+=layer.params\n",
        "            self.grads+=layer.grads\n",
        "\n",
        "    def predict(self, x): #순전파로 출력값 계산\n",
        "        for layer in self.layers:\n",
        "            x=layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, t): #input x와 label t를 받아 순전파를 수행하면서 loss 계산\n",
        "        score=self.predict(x)\n",
        "        loss=self.loss_layer.forward(score, t) #softmaxwithloss 계층\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1): #loss의 기울기인 dout을 받아 역전파를 수행하며 각 계층의 기울기 계산\n",
        "        dout=self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers): #역층 순으로 역전파 계산하며 기울기 전달\n",
        "            dout=layer.backward(dout)\n",
        "        return dout\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wr0GCh3bjhG"
      },
      "outputs": [],
      "source": [
        "def load_data(seed=1984):\n",
        "    np.random.seed(seed)\n",
        "    N = 100  # 클래스당 샘플 수\n",
        "    DIM = 2  # 데어터 요소 수\n",
        "    CLS_NUM = 3  # 클래스 수\n",
        "\n",
        "    x = np.zeros((N*CLS_NUM, DIM))\n",
        "    t = np.zeros((N*CLS_NUM, CLS_NUM), dtype=np.int)\n",
        "\n",
        "    for j in range(CLS_NUM):\n",
        "        for i in range(N): # N*j, N*(j+1)):\n",
        "            rate = i / N\n",
        "            radius = 1.0*rate\n",
        "            theta = j*4.0 + 4.0*rate + np.random.randn()*0.2\n",
        "\n",
        "            ix = N*j + i\n",
        "            x[ix] = np.array([radius*np.sin(theta),\n",
        "                              radius*np.cos(theta)]).flatten()\n",
        "            t[ix, j] = 1\n",
        "\n",
        "    return x, t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBXyfyrGbjhG",
        "outputId": "bec5264c-9285-4b47-d5b7-bff86810e491"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/LFC/.conda/envs/musicgen/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 10 / 10 | 손실 1.13\n",
            "| 에폭 2 |  반복 10 / 10 | 손실 1.13\n",
            "| 에폭 3 |  반복 10 / 10 | 손실 1.12\n",
            "| 에폭 4 |  반복 10 / 10 | 손실 1.12\n",
            "| 에폭 5 |  반복 10 / 10 | 손실 1.11\n",
            "| 에폭 6 |  반복 10 / 10 | 손실 1.14\n",
            "| 에폭 7 |  반복 10 / 10 | 손실 1.16\n",
            "| 에폭 8 |  반복 10 / 10 | 손실 1.11\n",
            "| 에폭 9 |  반복 10 / 10 | 손실 1.12\n",
            "| 에폭 10 |  반복 10 / 10 | 손실 1.13\n",
            "| 에폭 11 |  반복 10 / 10 | 손실 1.12\n",
            "| 에폭 12 |  반복 10 / 10 | 손실 1.11\n",
            "| 에폭 13 |  반복 10 / 10 | 손실 1.09\n",
            "| 에폭 14 |  반복 10 / 10 | 손실 1.08\n",
            "| 에폭 15 |  반복 10 / 10 | 손실 1.04\n",
            "| 에폭 16 |  반복 10 / 10 | 손실 1.03\n",
            "| 에폭 17 |  반복 10 / 10 | 손실 0.96\n",
            "| 에폭 18 |  반복 10 / 10 | 손실 0.92\n",
            "| 에폭 19 |  반복 10 / 10 | 손실 0.92\n",
            "| 에폭 20 |  반복 10 / 10 | 손실 0.87\n",
            "| 에폭 21 |  반복 10 / 10 | 손실 0.85\n",
            "| 에폭 22 |  반복 10 / 10 | 손실 0.82\n",
            "| 에폭 23 |  반복 10 / 10 | 손실 0.79\n",
            "| 에폭 24 |  반복 10 / 10 | 손실 0.78\n",
            "| 에폭 25 |  반복 10 / 10 | 손실 0.82\n",
            "| 에폭 26 |  반복 10 / 10 | 손실 0.78\n",
            "| 에폭 27 |  반복 10 / 10 | 손실 0.76\n",
            "| 에폭 28 |  반복 10 / 10 | 손실 0.76\n",
            "| 에폭 29 |  반복 10 / 10 | 손실 0.78\n",
            "| 에폭 30 |  반복 10 / 10 | 손실 0.75\n",
            "| 에폭 31 |  반복 10 / 10 | 손실 0.78\n",
            "| 에폭 32 |  반복 10 / 10 | 손실 0.77\n",
            "| 에폭 33 |  반복 10 / 10 | 손실 0.77\n",
            "| 에폭 34 |  반복 10 / 10 | 손실 0.78\n",
            "| 에폭 35 |  반복 10 / 10 | 손실 0.75\n",
            "| 에폭 36 |  반복 10 / 10 | 손실 0.74\n",
            "| 에폭 37 |  반복 10 / 10 | 손실 0.76\n",
            "| 에폭 38 |  반복 10 / 10 | 손실 0.76\n",
            "| 에폭 39 |  반복 10 / 10 | 손실 0.73\n",
            "| 에폭 40 |  반복 10 / 10 | 손실 0.75\n",
            "| 에폭 41 |  반복 10 / 10 | 손실 0.76\n",
            "| 에폭 42 |  반복 10 / 10 | 손실 0.76\n",
            "| 에폭 43 |  반복 10 / 10 | 손실 0.76\n",
            "| 에폭 44 |  반복 10 / 10 | 손실 0.74\n",
            "| 에폭 45 |  반복 10 / 10 | 손실 0.75\n",
            "| 에폭 46 |  반복 10 / 10 | 손실 0.73\n",
            "| 에폭 47 |  반복 10 / 10 | 손실 0.72\n",
            "| 에폭 48 |  반복 10 / 10 | 손실 0.73\n",
            "| 에폭 49 |  반복 10 / 10 | 손실 0.72\n",
            "| 에폭 50 |  반복 10 / 10 | 손실 0.72\n",
            "| 에폭 51 |  반복 10 / 10 | 손실 0.72\n",
            "| 에폭 52 |  반복 10 / 10 | 손실 0.72\n",
            "| 에폭 53 |  반복 10 / 10 | 손실 0.74\n",
            "| 에폭 54 |  반복 10 / 10 | 손실 0.74\n",
            "| 에폭 55 |  반복 10 / 10 | 손실 0.72\n",
            "| 에폭 56 |  반복 10 / 10 | 손실 0.72\n",
            "| 에폭 57 |  반복 10 / 10 | 손실 0.71\n",
            "| 에폭 58 |  반복 10 / 10 | 손실 0.70\n",
            "| 에폭 59 |  반복 10 / 10 | 손실 0.72\n",
            "| 에폭 60 |  반복 10 / 10 | 손실 0.70\n",
            "| 에폭 61 |  반복 10 / 10 | 손실 0.71\n",
            "| 에폭 62 |  반복 10 / 10 | 손실 0.72\n",
            "| 에폭 63 |  반복 10 / 10 | 손실 0.70\n",
            "| 에폭 64 |  반복 10 / 10 | 손실 0.71\n",
            "| 에폭 65 |  반복 10 / 10 | 손실 0.73\n",
            "| 에폭 66 |  반복 10 / 10 | 손실 0.70\n",
            "| 에폭 67 |  반복 10 / 10 | 손실 0.71\n",
            "| 에폭 68 |  반복 10 / 10 | 손실 0.69\n",
            "| 에폭 69 |  반복 10 / 10 | 손실 0.70\n",
            "| 에폭 70 |  반복 10 / 10 | 손실 0.71\n",
            "| 에폭 71 |  반복 10 / 10 | 손실 0.68\n",
            "| 에폭 72 |  반복 10 / 10 | 손실 0.69\n",
            "| 에폭 73 |  반복 10 / 10 | 손실 0.67\n",
            "| 에폭 74 |  반복 10 / 10 | 손실 0.68\n",
            "| 에폭 75 |  반복 10 / 10 | 손실 0.67\n",
            "| 에폭 76 |  반복 10 / 10 | 손실 0.66\n",
            "| 에폭 77 |  반복 10 / 10 | 손실 0.69\n",
            "| 에폭 78 |  반복 10 / 10 | 손실 0.64\n",
            "| 에폭 79 |  반복 10 / 10 | 손실 0.68\n",
            "| 에폭 80 |  반복 10 / 10 | 손실 0.64\n",
            "| 에폭 81 |  반복 10 / 10 | 손실 0.64\n",
            "| 에폭 82 |  반복 10 / 10 | 손실 0.66\n",
            "| 에폭 83 |  반복 10 / 10 | 손실 0.62\n",
            "| 에폭 84 |  반복 10 / 10 | 손실 0.62\n",
            "| 에폭 85 |  반복 10 / 10 | 손실 0.61\n",
            "| 에폭 86 |  반복 10 / 10 | 손실 0.60\n",
            "| 에폭 87 |  반복 10 / 10 | 손실 0.60\n",
            "| 에폭 88 |  반복 10 / 10 | 손실 0.61\n",
            "| 에폭 89 |  반복 10 / 10 | 손실 0.59\n",
            "| 에폭 90 |  반복 10 / 10 | 손실 0.58\n",
            "| 에폭 91 |  반복 10 / 10 | 손실 0.56\n",
            "| 에폭 92 |  반복 10 / 10 | 손실 0.56\n",
            "| 에폭 93 |  반복 10 / 10 | 손실 0.54\n",
            "| 에폭 94 |  반복 10 / 10 | 손실 0.53\n",
            "| 에폭 95 |  반복 10 / 10 | 손실 0.53\n",
            "| 에폭 96 |  반복 10 / 10 | 손실 0.52\n",
            "| 에폭 97 |  반복 10 / 10 | 손실 0.51\n",
            "| 에폭 98 |  반복 10 / 10 | 손실 0.50\n",
            "| 에폭 99 |  반복 10 / 10 | 손실 0.48\n",
            "| 에폭 100 |  반복 10 / 10 | 손실 0.48\n",
            "| 에폭 101 |  반복 10 / 10 | 손실 0.46\n",
            "| 에폭 102 |  반복 10 / 10 | 손실 0.45\n",
            "| 에폭 103 |  반복 10 / 10 | 손실 0.45\n",
            "| 에폭 104 |  반복 10 / 10 | 손실 0.44\n",
            "| 에폭 105 |  반복 10 / 10 | 손실 0.44\n",
            "| 에폭 106 |  반복 10 / 10 | 손실 0.41\n",
            "| 에폭 107 |  반복 10 / 10 | 손실 0.40\n",
            "| 에폭 108 |  반복 10 / 10 | 손실 0.41\n",
            "| 에폭 109 |  반복 10 / 10 | 손실 0.40\n",
            "| 에폭 110 |  반복 10 / 10 | 손실 0.40\n",
            "| 에폭 111 |  반복 10 / 10 | 손실 0.38\n",
            "| 에폭 112 |  반복 10 / 10 | 손실 0.38\n",
            "| 에폭 113 |  반복 10 / 10 | 손실 0.36\n",
            "| 에폭 114 |  반복 10 / 10 | 손실 0.37\n",
            "| 에폭 115 |  반복 10 / 10 | 손실 0.35\n",
            "| 에폭 116 |  반복 10 / 10 | 손실 0.34\n",
            "| 에폭 117 |  반복 10 / 10 | 손실 0.34\n",
            "| 에폭 118 |  반복 10 / 10 | 손실 0.34\n",
            "| 에폭 119 |  반복 10 / 10 | 손실 0.33\n",
            "| 에폭 120 |  반복 10 / 10 | 손실 0.34\n",
            "| 에폭 121 |  반복 10 / 10 | 손실 0.32\n",
            "| 에폭 122 |  반복 10 / 10 | 손실 0.32\n",
            "| 에폭 123 |  반복 10 / 10 | 손실 0.31\n",
            "| 에폭 124 |  반복 10 / 10 | 손실 0.31\n",
            "| 에폭 125 |  반복 10 / 10 | 손실 0.30\n",
            "| 에폭 126 |  반복 10 / 10 | 손실 0.30\n",
            "| 에폭 127 |  반복 10 / 10 | 손실 0.28\n",
            "| 에폭 128 |  반복 10 / 10 | 손실 0.28\n",
            "| 에폭 129 |  반복 10 / 10 | 손실 0.28\n",
            "| 에폭 130 |  반복 10 / 10 | 손실 0.28\n",
            "| 에폭 131 |  반복 10 / 10 | 손실 0.27\n",
            "| 에폭 132 |  반복 10 / 10 | 손실 0.27\n",
            "| 에폭 133 |  반복 10 / 10 | 손실 0.27\n",
            "| 에폭 134 |  반복 10 / 10 | 손실 0.27\n",
            "| 에폭 135 |  반복 10 / 10 | 손실 0.27\n",
            "| 에폭 136 |  반복 10 / 10 | 손실 0.26\n",
            "| 에폭 137 |  반복 10 / 10 | 손실 0.26\n",
            "| 에폭 138 |  반복 10 / 10 | 손실 0.26\n",
            "| 에폭 139 |  반복 10 / 10 | 손실 0.25\n",
            "| 에폭 140 |  반복 10 / 10 | 손실 0.24\n",
            "| 에폭 141 |  반복 10 / 10 | 손실 0.24\n",
            "| 에폭 142 |  반복 10 / 10 | 손실 0.25\n",
            "| 에폭 143 |  반복 10 / 10 | 손실 0.24\n",
            "| 에폭 144 |  반복 10 / 10 | 손실 0.24\n",
            "| 에폭 145 |  반복 10 / 10 | 손실 0.23\n",
            "| 에폭 146 |  반복 10 / 10 | 손실 0.24\n",
            "| 에폭 147 |  반복 10 / 10 | 손실 0.23\n",
            "| 에폭 148 |  반복 10 / 10 | 손실 0.23\n",
            "| 에폭 149 |  반복 10 / 10 | 손실 0.22\n",
            "| 에폭 150 |  반복 10 / 10 | 손실 0.22\n",
            "| 에폭 151 |  반복 10 / 10 | 손실 0.22\n",
            "| 에폭 152 |  반복 10 / 10 | 손실 0.22\n",
            "| 에폭 153 |  반복 10 / 10 | 손실 0.22\n",
            "| 에폭 154 |  반복 10 / 10 | 손실 0.22\n",
            "| 에폭 155 |  반복 10 / 10 | 손실 0.22\n",
            "| 에폭 156 |  반복 10 / 10 | 손실 0.21\n",
            "| 에폭 157 |  반복 10 / 10 | 손실 0.21\n",
            "| 에폭 158 |  반복 10 / 10 | 손실 0.20\n",
            "| 에폭 159 |  반복 10 / 10 | 손실 0.21\n",
            "| 에폭 160 |  반복 10 / 10 | 손실 0.20\n",
            "| 에폭 161 |  반복 10 / 10 | 손실 0.20\n",
            "| 에폭 162 |  반복 10 / 10 | 손실 0.20\n",
            "| 에폭 163 |  반복 10 / 10 | 손실 0.21\n",
            "| 에폭 164 |  반복 10 / 10 | 손실 0.20\n",
            "| 에폭 165 |  반복 10 / 10 | 손실 0.20\n",
            "| 에폭 166 |  반복 10 / 10 | 손실 0.19\n",
            "| 에폭 167 |  반복 10 / 10 | 손실 0.19\n",
            "| 에폭 168 |  반복 10 / 10 | 손실 0.19\n",
            "| 에폭 169 |  반복 10 / 10 | 손실 0.19\n",
            "| 에폭 170 |  반복 10 / 10 | 손실 0.19\n",
            "| 에폭 171 |  반복 10 / 10 | 손실 0.19\n",
            "| 에폭 172 |  반복 10 / 10 | 손실 0.18\n",
            "| 에폭 173 |  반복 10 / 10 | 손실 0.18\n",
            "| 에폭 174 |  반복 10 / 10 | 손실 0.18\n",
            "| 에폭 175 |  반복 10 / 10 | 손실 0.18\n",
            "| 에폭 176 |  반복 10 / 10 | 손실 0.18\n",
            "| 에폭 177 |  반복 10 / 10 | 손실 0.18\n",
            "| 에폭 178 |  반복 10 / 10 | 손실 0.18\n",
            "| 에폭 179 |  반복 10 / 10 | 손실 0.17\n",
            "| 에폭 180 |  반복 10 / 10 | 손실 0.17\n",
            "| 에폭 181 |  반복 10 / 10 | 손실 0.18\n",
            "| 에폭 182 |  반복 10 / 10 | 손실 0.17\n",
            "| 에폭 183 |  반복 10 / 10 | 손실 0.18\n",
            "| 에폭 184 |  반복 10 / 10 | 손실 0.17\n",
            "| 에폭 185 |  반복 10 / 10 | 손실 0.17\n",
            "| 에폭 186 |  반복 10 / 10 | 손실 0.18\n",
            "| 에폭 187 |  반복 10 / 10 | 손실 0.17\n",
            "| 에폭 188 |  반복 10 / 10 | 손실 0.17\n",
            "| 에폭 189 |  반복 10 / 10 | 손실 0.17\n",
            "| 에폭 190 |  반복 10 / 10 | 손실 0.17\n",
            "| 에폭 191 |  반복 10 / 10 | 손실 0.16\n",
            "| 에폭 192 |  반복 10 / 10 | 손실 0.17\n",
            "| 에폭 193 |  반복 10 / 10 | 손실 0.16\n",
            "| 에폭 194 |  반복 10 / 10 | 손실 0.16\n",
            "| 에폭 195 |  반복 10 / 10 | 손실 0.16\n",
            "| 에폭 196 |  반복 10 / 10 | 손실 0.16\n",
            "| 에폭 197 |  반복 10 / 10 | 손실 0.16\n",
            "| 에폭 198 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 199 |  반복 10 / 10 | 손실 0.16\n",
            "| 에폭 200 |  반복 10 / 10 | 손실 0.16\n",
            "| 에폭 201 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 202 |  반복 10 / 10 | 손실 0.16\n",
            "| 에폭 203 |  반복 10 / 10 | 손실 0.16\n",
            "| 에폭 204 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 205 |  반복 10 / 10 | 손실 0.16\n",
            "| 에폭 206 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 207 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 208 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 209 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 210 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 211 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 212 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 213 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 214 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 215 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 216 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 217 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 218 |  반복 10 / 10 | 손실 0.15\n",
            "| 에폭 219 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 220 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 221 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 222 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 223 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 224 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 225 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 226 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 227 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 228 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 229 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 230 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 231 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 232 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 233 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 234 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 235 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 236 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 237 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 238 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 239 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 240 |  반복 10 / 10 | 손실 0.14\n",
            "| 에폭 241 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 242 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 243 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 244 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 245 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 246 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 247 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 248 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 249 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 250 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 251 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 252 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 253 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 254 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 255 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 256 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 257 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 258 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 259 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 260 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 261 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 262 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 263 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 264 |  반복 10 / 10 | 손실 0.13\n",
            "| 에폭 265 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 266 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 267 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 268 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 269 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 270 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 271 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 272 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 273 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 274 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 275 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 276 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 277 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 278 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 279 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 280 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 281 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 282 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 283 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 284 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 285 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 286 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 287 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 288 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 289 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 290 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 291 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 292 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 293 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 294 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 295 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 296 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 297 |  반복 10 / 10 | 손실 0.12\n",
            "| 에폭 298 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 299 |  반복 10 / 10 | 손실 0.11\n",
            "| 에폭 300 |  반복 10 / 10 | 손실 0.11\n"
          ]
        }
      ],
      "source": [
        "#train code\n",
        "\n",
        "#하이퍼파라미터 설정\n",
        "max_epoch=300\n",
        "batch_size=30\n",
        "hidden_size=10\n",
        "learning_rate=1.0\n",
        "\n",
        "#데이터 읽기, 모델과 옵티마이저 생성\n",
        "x, t=load_data()\n",
        "model=TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
        "optimizer=SGD(lr=learning_rate)\n",
        "\n",
        "#학습에 사용하는 변수\n",
        "data_size=len(x)\n",
        "max_iters=data_size//batch_size\n",
        "total_loss=0\n",
        "loss_count=0\n",
        "loss_list=[]\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    #데이터 shuffle\n",
        "    idx=np.random.permutation(data_size)\n",
        "    x=x[idx]\n",
        "    t=t[idx]\n",
        "\n",
        "    for iters in range(max_iters):\n",
        "        batch_x=x[iters*batch_size:(iters+1)*batch_size]\n",
        "        batch_t=t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "        #기울기를 구하여 파라미터 갱신\n",
        "        loss=model.forward(batch_x, batch_t)\n",
        "        model.backward()\n",
        "        optimizer.update(model.params, model.grads)\n",
        "\n",
        "        total_loss+=loss\n",
        "        loss_count+=1\n",
        "\n",
        "        #정기적으로 학습 결과 출력\n",
        "        if (iters+1)%10==0:\n",
        "            avg_loss=total_loss/loss_count\n",
        "            print('| 에폭 %d |  반복 %d / %d | 손실 %.2f' % (epoch + 1, iters + 1, max_iters, avg_loss))\n",
        "            loss_list.append(avg_loss)\n",
        "            total_loss, loss_count=0,0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0p5UGBFbjhH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "musicgen",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}