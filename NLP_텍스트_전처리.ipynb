{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH9umji+GQcqwqrEuH5df4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UiinKim/UiinKim/blob/main/NLP_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  1. 토큰화 -> 토큰으로 나누는 기준마다 다르다(단어, 문장 등)"
      ],
      "metadata": {
        "id": "KxQXxKrW5l2L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wzHKPh7cWuP0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3slP54Q9plPB",
        "outputId": "b995e783-a3c3-4445-fe60-dc8957cb7f8c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 토큰화에서는 단순 띄어쓰기, 어퍼스트로피, 특수문자로 구분해서는 안된다.\n",
        "sentence=\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
        "print(word_tokenize(sentence))\n",
        "print(WordPunctTokenizer().tokenize(sentence))\n",
        "print(text_to_word_sequence(sentence))\n",
        "print(TreebankWordTokenizer().tokenize(sentence))\n",
        "#토큰화 도구마다 말뭉치(코퍼스)가 다르다\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_H4kbuuW4hd",
        "outputId": "27cdfe3e-d159-4a4b-e204-889aae891dc2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문장 토큰화에서는 '.'으로 구분하면 ip주소나 이메일에서 걸린다.\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print(sent_tokenize(text))\n",
        "#밑에는 마침표가 여러개 등장하는 경우\n",
        "text_cf=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print(sent_tokenize(text_cf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ditc-tzvpRNr",
        "outputId": "9245be22-d770-45fb-e690-ec9edfede03b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n",
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss\n",
        "#kss는 한국어 문장 토큰화 도구"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvTiFNjMryi7",
        "outputId": "47f93beb-be7d-41d6-eb61-bcc74eed86a0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-4.5.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji==1.2.0 (from kss)\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from kss) (2022.10.31)\n",
            "Collecting pecab (from kss)\n",
            "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (1.22.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (9.0.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (7.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.1)\n",
            "Building wheels for collected packages: kss, pecab\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-4.5.3-py3-none-any.whl size=54258 sha256=39b38cf9401891608045d0c86ee56e707ebb19c79c83454b71fe4a1ca5c0ce52\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/9e/a3/5b09e3f14722fa0d77f47fe840668d426760023bdd11b0fbd9\n",
            "  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646666 sha256=4a6940326730ebaa84fb60d1e9968428db5b54086fd8bcd14f531ba856e0cfb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/6f/b4/ab61b8863d7d8b1409def8ae31adcaa089fa91b8d022ec309d\n",
            "Successfully built kss pecab\n",
            "Installing collected packages: emoji, pecab, kss\n",
            "Successfully installed emoji-1.2.0 kss-4.5.3 pecab-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB9nwQEgucu8",
        "outputId": "0d33fd25-98b6-4299-e51a-47015f56472c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "\n",
        "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))\n",
        "#한국어는 영어와 달리 어절 토큰화가 아닌 형태소 토큰화를 시켜야한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EItIkL6bsR3b",
        "outputId": "99fcf9f2-d912-4d79-8817-9e324427f360"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "품사태깅 한국어의 '못', 영어의 'fly' -> 의미를 명확하게 알기 위해서 사용"
      ],
      "metadata": {
        "id": "IYmscEVq5h8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "tokenized_sentence=word_tokenize(text)\n",
        "print('단어 토큰화 : {}'.format(tokenized_sentence))\n",
        "print('품사 태깅 : {}'.format(pos_tag(tokenized_sentence)))\n",
        "#PRP는 인칭대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유명사, NNS는 복수형 명사, CC는 접속사, DT는 관사"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k9pkn_WseNz",
        "outputId": "5e0af579-ec4a-4145-fa77-345f25670e68"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFQoxsUgvBsE",
        "outputId": "070d1a42-4d05-4b05-e6e9-632af6567f9c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "okt=Okt()\n",
        "kkma=Kkma()\n",
        "\n",
        "text=\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "\n",
        "print('OKT 형태소 분석 : {}'.format(okt.morphs(text)))\n",
        "print('Kkma 형태소 분석 : {}'.format(kkma.morphs(text)))\n",
        "print('OKT 품사 태깅 : {}'.format(okt.pos(text)))\n",
        "print('Kkma 품사 태깅 : {}'.format(kkma.pos(text)))\n",
        "print('OKT 명사 추출 : {}'.format(okt.nouns(text)))\n",
        "print('Kkma 명사 추출 : {}'.format(kkma.nouns(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcwVOD6ytqrb",
        "outputId": "2c3b83ed-6b56-43e9-d0b3-c65087873935"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
            "Kkma 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
            "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
            "Kkma 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
            "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n",
            "Kkma 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2.정제(코퍼스에서 노이즈 데이터--> 등장 빈도가 낮은 단어, 길이가 짧은 단어 제거) 및 정규화(표현 방법이 다른 단어들을 통합)"
      ],
      "metadata": {
        "id": "spT0Ji0l5e2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re #regular expression 정규표현식\n",
        "text=\"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "\n",
        "#단어의 길이 1~2인 단어를 정규표현식으로 삭제\n",
        "shortword=re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "#compile 정규식 개체 리턴\n",
        "print(shortword.sub('',text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLbIp8G3vAM8",
        "outputId": "233a4a0c-b139-46ad-9df0-bd2ecab68b5b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6f-M1Ys0St6",
        "outputId": "9407c649-05d1-48f7-cebb-803ec2e6eb7a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 정규화 기법 : 어간 추출, 표제어(사전에 있는 단어) 추출 --> 하나의 단어로 일반화 시킬 수 있으면 문서 내의 단어의 수를 줄이겠다 -> BoW(Bag of Word)에서 사용\n",
        "\n",
        " 표제어 추출"
      ],
      "metadata": {
        "id": "cpdNIr8M5YEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#표제어(Lemma) : are, is, am -> be\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "\n",
        "print('표제어 추출 전 : {}'.format(words))\n",
        "print('표제어 추출 후 : {}'.format([lemmatizer.lemmatize(word) for word in words]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US312299yo_E",
        "outputId": "fab54492-55c4-403f-af02-966352b96ac3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WordNetLemmatizer은 품사를 알려주면 품사의 정보를 보존하며 정확학 Lemma가 추출된다.\n",
        "print(lemmatizer.lemmatize('dies', 'v'))\n",
        "print(lemmatizer.lemmatize('has', 'v'))\n",
        "print(lemmatizer.lemmatize('watched', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1zfpU8h0EpW",
        "outputId": "47489577-70bc-442a-ed1b-30f3a971b9fa"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "die\n",
            "have\n",
            "watch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어간(Stem) 추출"
      ],
      "metadata": {
        "id": "OyL8VFlD5Shy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer=PorterStemmer()\n",
        "\n",
        "sentence=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "tokenized_sentence=word_tokenize(sentence)\n",
        "print('어간 추출 전 : {}'.format(tokenized_sentence))\n",
        "print('어간 추출 후 : {}'.format([stemmer.stem(word) for word in tokenized_sentence]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4qdFg7507td",
        "outputId": "9c40e80e-fe34-4b91-fcca-9af003adda56"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
            "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer=PorterStemmer()\n",
        "lancaster_stemmer=LancasterStemmer()\n",
        "\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print('포터스태머 : ', [porter_stemmer.stem(word) for word in words])\n",
        "print('랭커스태머 : ', [lancaster_stemmer.stem(word) for word in words])\n",
        "#알고리즘이 다르기 때문에 매우 다른 결과가 나온다. 표제어 추출과도 매우 다르게 나온다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJMZgg7414qG",
        "outputId": "8aadc141-4f60-43b7-e61b-e0c0eb73c8cd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "포터스태머 :  ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
            "랭커스태머 :  ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 불용어 : 자주 등장하지만 분석을 하는데에 불필요한 단어\n"
      ],
      "metadata": {
        "id": "d2dpA_WA5Lpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "lr1Nl1fO5K4f"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpeeOYL06YR0",
        "outputId": "3f5e3a73-1664-4700-c75a-57ac853cc94b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_word_list=stopwords.words('english')\n",
        "print('불용어의 개수 : {}'.format(len(stop_word_list)))\n",
        "print('영어의 불용어 10개 출력 : {}'.format(stop_word_list[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuvY202F5NA_",
        "outputId": "35ae9f77-b087-4ab8-c274-005fce377fdf"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어의 개수 : 179\n",
            "영어의 불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK를 통해 불용어 제거하기\n",
        "example=\"Family is not an important thing. It's everything.\"\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "tokenized_sentence=word_tokenize(example)\n",
        "\n",
        "result=[]\n",
        "for word in tokenized_sentence:\n",
        "  if word not in stop_words:\n",
        "    result.append(word)\n",
        "\n",
        "print(\"불용어 제거 전 : {}\".format(tokenized_sentence))\n",
        "print('불용어 제거 후 : {}'.format(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5hTa_YL6W1Q",
        "outputId": "daa344df-1750-4882-b9ec-9a43e3f93d68"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#한국어 불용어 제거하기(절대적인 기준이 아니므로 사용자 설정을 하는 경우가 많음)\n",
        "okt=Okt()\n",
        "example=\"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words=\"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "\n",
        "stop_words=set(stop_words.split(' '))\n",
        "word_tokens=okt.morphs(example)\n",
        "\n",
        "result=[word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print('불용어 제거 전 : {}'.format(word_tokens))\n",
        "print('불용어 제거 후 : {}'.format(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnsTcB6g7PAA",
        "outputId": "85a44a9e-3b78-426f-a699-525689656ac5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 정규표현식 -> re와 nltk를 통해 토큰화하는데 유용\n",
        "\n"
      ],
      "metadata": {
        "id": "tT7oRjUP8atd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "r=re.compile('a.c')\n",
        "r.search('kkk')\n",
        "#아무것도 출력되지 않는다."
      ],
      "metadata": {
        "id": "xhIQapPT8NM4"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search('abc')\n",
        "# '.'은 한 개의 임의의 문자를 나타낸다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSWgR2Sm9od7",
        "outputId": "49304e38-1dc1-498a-8537-86febf63b6dd"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r=re.compile('ab?c')\n",
        "r.search('abbbc')\n",
        "#아무것도 출력되지 않는다."
      ],
      "metadata": {
        "id": "2rTZIdEy9MHg"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search('abc')\n",
        "# '?'앞의 문자인 b가 존재할 수도 있고 존재하지 않을 수도 있다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaHzX55S9Nrw",
        "outputId": "a67866c3-7627-4ec4-d33b-729ed464727d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search('ac')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q7aFbDs9rkX",
        "outputId": "252f4a2b-067c-4aa0-ebd6-3e4d5c63724e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ac'>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# . : 대체할 수 있다.\n",
        "# ? : ? 앞의 문자가 존재할 수도 있고 존재하지 않을 수도 있다.\n",
        "# * : * 앞의 문자가 0개 이상이다.\n",
        "# + : + 앞의 문자가 1개 이상이다.\n",
        "# ^ : ^ 뒤의 문자열이 모두 포함하여 시작되는 경우 매치한다.\n",
        "# {숫자} : {숫자} 앞의 문자를 숫자만큼 반복하여 나타거나 매치한다.\n",
        "# {숫자1, 숫자2} : {숫자 1, 숫자 2} 앞의 문자를 숫자 1과 숫자 2 사이만큼 반복하여 나타내거나 매치한다.\n",
        "# {숫자, } : {숫자} 앞의 문자를 숫자 이상만큼 반복한다\n",
        "# [] : [] 안의 문자들 중 한개의 문자와 매치한다. [A-Za-z]는 모든 알파벳을 의미하고 [0-9]는 숫자 전부를 의미한다.\n",
        "# [^문자] : [^문자]를 제외한 모든 문자를 매치한다. [^abc]이면 a or b or c를 제외한 모든 문자\n"
      ],
      "metadata": {
        "id": "0jb87yhB95Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zbrWztG9r_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}