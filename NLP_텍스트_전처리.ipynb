{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpKq7paa75KBFNiubVZfn2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UiinKim/UiinKim/blob/main/NLP_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  1. 토큰화 -> 토큰으로 나누는 기준마다 다르다(단어, 문장 등)"
      ],
      "metadata": {
        "id": "KxQXxKrW5l2L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "wzHKPh7cWuP0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3slP54Q9plPB",
        "outputId": "6c9332f0-6fe4-4bee-8e1f-bb6983cd8182"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 토큰화에서는 단순 띄어쓰기, 어퍼스트로피, 특수문자로 구분해서는 안된다.\n",
        "sentence=\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
        "print(word_tokenize(sentence))\n",
        "print(WordPunctTokenizer().tokenize(sentence))\n",
        "print(text_to_word_sequence(sentence))\n",
        "print(TreebankWordTokenizer().tokenize(sentence))\n",
        "#토큰화 도구마다 말뭉치(코퍼스)가 다르다\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_H4kbuuW4hd",
        "outputId": "31d75932-68c3-4915-f27a-6baedfef2440"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문장 토큰화에서는 '.'으로 구분하면 ip주소나 이메일에서 걸린다.\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print(sent_tokenize(text))\n",
        "#밑에는 마침표가 여러개 등장하는 경우\n",
        "text_cf=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print(sent_tokenize(text_cf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ditc-tzvpRNr",
        "outputId": "76e84e23-b91b-43c1-fb57-e399e498ea8d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n",
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss\n",
        "#kss는 한국어 문장 토큰화 도구"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvTiFNjMryi7",
        "outputId": "01d16b1f-ff42-4533-b439-2c0889421cf8"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kss in /usr/local/lib/python3.10/dist-packages (4.5.3)\n",
            "Requirement already satisfied: emoji==1.2.0 in /usr/local/lib/python3.10/dist-packages (from kss) (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from kss) (2022.10.31)\n",
            "Requirement already satisfied: pecab in /usr/local/lib/python3.10/dist-packages (from kss) (1.0.8)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (1.22.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (9.0.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (7.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.1.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB9nwQEgucu8",
        "outputId": "d0b3ee56-c768-44bd-a23d-6eb56ccd32d8"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "\n",
        "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))\n",
        "#한국어는 영어와 달리 어절 토큰화가 아닌 형태소 토큰화를 시켜야한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EItIkL6bsR3b",
        "outputId": "1449c135-64f6-4a1e-ae86-1e98bea6b50a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "품사태깅 한국어의 '못', 영어의 'fly' -> 의미를 명확하게 알기 위해서 사용"
      ],
      "metadata": {
        "id": "IYmscEVq5h8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "tokenized_sentence=word_tokenize(text)\n",
        "print('단어 토큰화 : {}'.format(tokenized_sentence))\n",
        "print('품사 태깅 : {}'.format(pos_tag(tokenized_sentence)))\n",
        "#PRP는 인칭대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유명사, NNS는 복수형 명사, CC는 접속사, DT는 관사"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k9pkn_WseNz",
        "outputId": "a79e8c99-2afb-4971-d93f-b20653627b86"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFQoxsUgvBsE",
        "outputId": "e95fd844-43ac-4b20-8fd8-c1f92a974ac0"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "okt=Okt()\n",
        "kkma=Kkma()\n",
        "\n",
        "text=\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "\n",
        "print('OKT 형태소 분석 : {}'.format(okt.morphs(text)))\n",
        "print('Kkma 형태소 분석 : {}'.format(kkma.morphs(text)))\n",
        "print('OKT 품사 태깅 : {}'.format(okt.pos(text)))\n",
        "print('Kkma 품사 태깅 : {}'.format(kkma.pos(text)))\n",
        "print('OKT 명사 추출 : {}'.format(okt.nouns(text)))\n",
        "print('Kkma 명사 추출 : {}'.format(kkma.nouns(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcwVOD6ytqrb",
        "outputId": "d33e47d4-3fd3-4703-aa84-ac3a68308754"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
            "Kkma 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
            "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
            "Kkma 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
            "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n",
            "Kkma 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2.정제(코퍼스에서 노이즈 데이터--> 등장 빈도가 낮은 단어, 길이가 짧은 단어 제거) 및 정규화(표현 방법이 다른 단어들을 통합)"
      ],
      "metadata": {
        "id": "spT0Ji0l5e2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re #regular expression 정규표현식\n",
        "text=\"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "\n",
        "#단어의 길이 1~2인 단어를 정규표현식으로 삭제\n",
        "shortword=re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "#compile 정규식 개체 리턴\n",
        "print(shortword.sub('',text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLbIp8G3vAM8",
        "outputId": "f9b5b2e2-bf69-432f-f8f9-e40f5f44cd87"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6f-M1Ys0St6",
        "outputId": "6c6a5644-50e9-4328-99c4-8edcbc381aaf"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 정규화 기법 : 어간 추출, 표제어(사전에 있는 단어) 추출 --> 하나의 단어로 일반화 시킬 수 있으면 문서 내의 단어의 수를 줄이겠다 -> BoW(Bag of Word)에서 사용\n",
        "\n",
        " 표제어 추출"
      ],
      "metadata": {
        "id": "cpdNIr8M5YEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#표제어(Lemma) : are, is, am -> be\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "\n",
        "print('표제어 추출 전 : {}'.format(words))\n",
        "print('표제어 추출 후 : {}'.format([lemmatizer.lemmatize(word) for word in words]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US312299yo_E",
        "outputId": "e38e9268-21cc-43f6-8856-5a6a8d81d6cf"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WordNetLemmatizer은 품사를 알려주면 품사의 정보를 보존하며 정확학 Lemma가 추출된다.\n",
        "print(lemmatizer.lemmatize('dies', 'v'))\n",
        "print(lemmatizer.lemmatize('has', 'v'))\n",
        "print(lemmatizer.lemmatize('watched', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1zfpU8h0EpW",
        "outputId": "e142e03f-868e-4a87-f9c4-afc028b00b26"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "die\n",
            "have\n",
            "watch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어간(Stem) 추출"
      ],
      "metadata": {
        "id": "OyL8VFlD5Shy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer=PorterStemmer()\n",
        "\n",
        "sentence=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "tokenized_sentence=word_tokenize(sentence)\n",
        "print('어간 추출 전 : {}'.format(tokenized_sentence))\n",
        "print('어간 추출 후 : {}'.format([stemmer.stem(word) for word in tokenized_sentence]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4qdFg7507td",
        "outputId": "576cc537-8c72-46b9-f7ca-616479d6411b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
            "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer=PorterStemmer()\n",
        "lancaster_stemmer=LancasterStemmer()\n",
        "\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print('포터스태머 : ', [porter_stemmer.stem(word) for word in words])\n",
        "print('랭커스태머 : ', [lancaster_stemmer.stem(word) for word in words])\n",
        "#알고리즘이 다르기 때문에 매우 다른 결과가 나온다. 표제어 추출과도 매우 다르게 나온다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJMZgg7414qG",
        "outputId": "8f826ac7-9a1a-468a-934c-e78b7866752a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "포터스태머 :  ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
            "랭커스태머 :  ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 불용어 : 자주 등장하지만 분석을 하는데에 불필요한 단어\n"
      ],
      "metadata": {
        "id": "d2dpA_WA5Lpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "lr1Nl1fO5K4f"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpeeOYL06YR0",
        "outputId": "cd69e87f-476c-4de6-aaf6-0dac15ff38d0"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_word_list=stopwords.words('english')\n",
        "print('불용어의 개수 : {}'.format(len(stop_word_list)))\n",
        "print('영어의 불용어 10개 출력 : {}'.format(stop_word_list[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuvY202F5NA_",
        "outputId": "2bcfa540-854f-4cad-8f30-de2f1d1d7f7d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어의 개수 : 179\n",
            "영어의 불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK를 통해 불용어 제거하기\n",
        "example=\"Family is not an important thing. It's everything.\"\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "tokenized_sentence=word_tokenize(example)\n",
        "\n",
        "result=[]\n",
        "for word in tokenized_sentence:\n",
        "  if word not in stop_words:\n",
        "    result.append(word)\n",
        "\n",
        "print(\"불용어 제거 전 : {}\".format(tokenized_sentence))\n",
        "print('불용어 제거 후 : {}'.format(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5hTa_YL6W1Q",
        "outputId": "be9af0a5-c8ab-4ddf-91b8-72f08ce26674"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#한국어 불용어 제거하기(절대적인 기준이 아니므로 사용자 설정을 하는 경우가 많음)\n",
        "okt=Okt()\n",
        "example=\"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words=\"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "\n",
        "stop_words=set(stop_words.split(' '))\n",
        "word_tokens=okt.morphs(example)\n",
        "\n",
        "result=[word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print('불용어 제거 전 : {}'.format(word_tokens))\n",
        "print('불용어 제거 후 : {}'.format(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnsTcB6g7PAA",
        "outputId": "5fbb493b-813e-4e92-b40b-e21ded9ef5b9"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 정규표현식 -> re와 nltk를 통해 토큰화하는데 유용\n",
        "\n"
      ],
      "metadata": {
        "id": "tT7oRjUP8atd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "r=re.compile('a.c')\n",
        "r.search('kkk')\n",
        "#아무것도 출력되지 않는다."
      ],
      "metadata": {
        "id": "xhIQapPT8NM4"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search('abc')\n",
        "# '.'은 한 개의 임의의 문자를 나타낸다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSWgR2Sm9od7",
        "outputId": "b7b2be01-7435-409a-c0a6-c4d87de93ac2"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r=re.compile('ab?c')\n",
        "r.search('abbbc')\n",
        "#아무것도 출력되지 않는다."
      ],
      "metadata": {
        "id": "2rTZIdEy9MHg"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search('abc')\n",
        "# '?'앞의 문자인 b가 존재할 수도 있고 존재하지 않을 수도 있다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaHzX55S9Nrw",
        "outputId": "86e76189-380c-49e3-d5e9-6dd3c5968ffc"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search('ac')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q7aFbDs9rkX",
        "outputId": "966edfa5-147e-4643-e9ac-1a03e2f7d3ca"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ac'>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# . : 대체할 수 있다.\n",
        "# ? : ? 앞의 문자가 존재할 수도 있고 존재하지 않을 수도 있다.\n",
        "# * : * 앞의 문자가 0개 이상이다.\n",
        "# + : + 앞의 문자가 1개 이상이다.\n",
        "# ^ : ^ 뒤의 문자열이 모두 포함하여 시작되는 경우 매치한다.\n",
        "# {숫자} : {숫자} 앞의 문자를 숫자만큼 반복하여 나타거나 매치한다.\n",
        "# {숫자1, 숫자2} : {숫자 1, 숫자 2} 앞의 문자를 숫자 1과 숫자 2 사이만큼 반복하여 나타내거나 매치한다.\n",
        "# {숫자, } : {숫자} 앞의 문자를 숫자 이상만큼 반복한다\n",
        "# [] : [] 안의 문자들 중 한개의 문자와 매치한다. [A-Za-z]는 모든 알파벳을 의미하고 [0-9]는 숫자 전부를 의미한다.\n",
        "# [^문자] : [^문자]를 제외한 모든 문자를 매치한다. [^abc]이면 a or b or c를 제외한 모든 문자\n"
      ],
      "metadata": {
        "id": "0jb87yhB95Nf"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 정수 인코딩(정수와 텍스트를 맵핑)\n",
        "6-1. dictionary 사용"
      ],
      "metadata": {
        "id": "h7jnnLJUc3Ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "2zbrWztG9r_d"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text=\"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "metadata": {
        "id": "z4-nsFm9dd4Z"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#문장 토큰화\n",
        "sentences=sent_tokenize(raw_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7kE5IMpdkKo",
        "outputId": "13a6fc0c-b22e-452b-9092-a53d88b08c25"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 정제(불용어)\n",
        "vocab={}\n",
        "preprocessed_sentence=[]\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "  #단어 토큰화\n",
        "  tokenized_sentence=word_tokenize(sentence)\n",
        "  result=[]\n",
        "\n",
        "  for word in tokenized_sentence:\n",
        "    word=word.lower() #모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
        "    if word not in stop_words: #단어 토큰화 된 결과에 대해 불용어를 제거한다.\n",
        "      if len(word) >2: #단어의 길이가 2 이하인 경우 추가로 단어를 제거한다.\n",
        "        result.append(word)\n",
        "        if word not in vocab:\n",
        "          vocab[word]=0 #처음 등장인 경우 해당 단어를 0으로 초기화\n",
        "        vocab[word]+=1 #등장한 횟수만큼 +1이 된다.\n",
        "  preprocessed_sentence.append(result) #불용어를 제거한 단어들만 추가한다.\n",
        "print(preprocessed_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHddX14sd6TI",
        "outputId": "1a023f0d-276f-4679-877d-af0d627b2086"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 집합 : ', vocab)\n",
        "print('barber의 빈도 수 : ', vocab['barber'])\n",
        "#빈도수 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAuNi_TQgPRR",
        "outputId": "b9c38601-f1ef-48f7-c33d-8921e912ba56"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 :  {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
            "barber의 빈도 수 :  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#빈도수대로 정렬\n",
        "vocab_sorted=sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
        "# vocab.items()는 vocab의 key와 value를 모두 포함한다는 뜻이고 lambda는 새로운 x로 설정하고 싶은 값을 x:원하는 값으로 설정한다.\n",
        "print(vocab_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqrPmKMNggdp",
        "outputId": "98b3ba49-bfa5-4370-dc4b-77d164b81dc7"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={}\n",
        "i=0 # 높은 빈도수를 가진 단어부터 1부터 입력\n",
        "for(word, frequency) in vocab_sorted:\n",
        "  if frequency > 1: #빈도수가 1회 이하이면 제외한다.\n",
        "    i=i+1\n",
        "    word_to_index[word]=i\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZELzNd5g3Vp",
        "outputId": "5c26fa62-fa6d-42dd-b61f-d05b5f808e1a"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=5\n",
        "words_frequency=[word for word,index in word_to_index.items() if index>=vocab_size+1]\n",
        "#인덱스가 5보다 높은 단어 들만 리스트에 저장\n",
        "for word in words_frequency:\n",
        "  del word_to_index[word] #word_frequency에 포함된 단어 제거\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9ks2XMTka9F",
        "outputId": "c5f05916-5e93-4f7d-a932-bf3997ef71aa"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어들을 정수로 인코딩\n",
        "#단어 집합에 존재하지 않는 단어들을 Out-Of_Vocabulary(OOV)라고 한다.\n",
        "word_to_index['OOV']=len(word_to_index)+1 #OOV의 단어의 경우를 추가해준다.\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojvn7xpflqlr",
        "outputId": "b4c90b38-cfcd-46e3-da3e-bd352d43a135"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word_to_index를 사용하여 sentences의 모든 단어들을 맵핑되는 정수로 인코딩\n",
        "encoded_sentences=[]\n",
        "for sentence in preprocessed_sentence:\n",
        "  encoded_sentence=[]\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      #단어 집합에 있는 단어이면 해당 단어의 정수를 리턴.\n",
        "      encoded_sentence.append(word_to_index[word])\n",
        "    except KeyError:\n",
        "      #없는 경우 'OOV'를 리턴\n",
        "      encoded_sentence.append(word_to_index['OOV'])\n",
        "  encoded_sentences.append(encoded_sentence)\n",
        "print(encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nopBKAm_mQ-z",
        "outputId": "debecf29-d524-43b6-c048-9f9cf4d41b04"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Counter 사용하기"
      ],
      "metadata": {
        "id": "nETOpmotnrmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter #위의 dictionary로 일일이 저장하는 방법을 api로 불러와 간단하게 하는 방법\n",
        "print(preprocessed_sentence)\n",
        "#prerprocessed_sentence는 단어 토큰화의 결과만 저장되어 있어 단어 집합을 만들기 위해 ','를 제거하고 단어들을 하나의 리스트로 만든다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKFe1Ur4nO2T",
        "outputId": "e298c70f-fb2e-4db9-931b-eef4def700b0"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words_list=sum(preprocessed_sentence, [])\n",
        "print(all_words_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q7sNXC1oTys",
        "outputId": "2ee8933f-9478-4ec6-dd3c-13772a14e748"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=Counter(all_words_list) #자동으로 빈도수에 따른 dictionary화 및 정렬\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0_7VM1LpcJc",
        "outputId": "878cff03-ea5f-4cce-aa70-ef51a7192e9a"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=5\n",
        "vocab=vocab.most_common(vocab_size) #vocab_size만큼 가장 빈도수가 많은 것만 저장\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMA0nGa_pj0r",
        "outputId": "95f36e1a-5021-4714-ce5f-7efed8a030e2"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={}\n",
        "i=0\n",
        "for(word, frequency) in vocab:\n",
        "  i=i+1\n",
        "  word_to_index[word]=i\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFfCumU4pr1D",
        "outputId": "efe1c639-9c02-4e8b-8677-924d16fc4388"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6-3. NLTK의 FreqDist 사용하기"
      ],
      "metadata": {
        "id": "KnJPew8uq4lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jn13e2aKqHHN"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hstack은 행이 같으면 모든 리스트들을 column옆으로 쭉 추가한다. -> 문장 구분을 제거한다.\n",
        "vocab=FreqDist(np.hstack(preprocessed_sentence)) #Counter와 같은 방법이다."
      ],
      "metadata": {
        "id": "gxo2Vcwtq_51"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab['barber'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dxFKzMzrQaj",
        "outputId": "b64a419e-6326-4b75-bb50-827f7269fcd8"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=5\n",
        "vocab=vocab.most_common(vocab_size)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF0Rll9erSvU",
        "outputId": "58fe24a4-af4b-4b9e-c8cb-517169b29aa4"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppQrJOPttVDW"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#enumerate는 (인덱스, value)로 변환된다. 여기서 인덱스를 자동으로 구해주는 함수이다.\n",
        "word_to_index={word[0]:index+1 for index, word in enumerate(vocab)}\n",
        "#word는 vocab의 1튜플씩 나오며 word[0]은 (단어, 빈도수) 중에 단어에 해당한다. index는 enumerate를 통해 인덱스값이 추출 되며 +1을 하여 정수로 지정한다.\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRQz-a3Srf8M",
        "outputId": "aa08621d-896d-41a8-891d-b78e5af4cabc"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#enumerate 실습\n",
        "#enumerate는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 리턴\n",
        "test_input=['a', 'b', 'c', 'd', 'e']\n",
        "for index, value in enumerate(test_input):\n",
        "  print(\"value : {}, index : {}\".format(value, index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0AUgfh1tfU1",
        "outputId": "107671bd-2b72-4f94-8093-b350a6309b64"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value : a, index : 0\n",
            "value : b, index : 1\n",
            "value : c, index : 2\n",
            "value : d, index : 3\n",
            "value : e, index : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6-4. 케라스(Keras) 텍스트 전처리"
      ],
      "metadata": {
        "id": "ID6jW2fXuXDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "preprocessed_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1gLgy9SuTQO",
        "outputId": "574c86dc-7e02-4037-eb2d-2ea4a2388a51"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['barber', 'person'],\n",
              " ['barber', 'good', 'person'],\n",
              " ['barber', 'huge', 'person'],\n",
              " ['knew', 'secret'],\n",
              " ['secret', 'kept', 'huge', 'secret'],\n",
              " ['huge', 'secret'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'secret'],\n",
              " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
              " ['barber', 'went', 'huge', 'mountain']]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "\n",
        "#fit_on_texts() 안에 코퍼스(preprocessed_sentence)를 입력으로 하면 빈도수를 기준으로 단어 집합 생성\n",
        "tokenizer.fit_on_texts(preprocessed_sentence) #각 문장별로 총 단어 빈도수가 높은 순으로 낮은 인덱스를 부여한다.\n",
        "print(tokenizer.word_index) #인덱스가 어떻게 부여되었는지 확인한다.\n",
        "print(tokenizer.word_counts) #각 단어가 몇 개 였는지 확인한다.\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentence)) #입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTXkjCv3uruN",
        "outputId": "8072db9e-02eb-441c-c4d6-090fa8e42a4c"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=5\n",
        "tokenizer=Tokenizer(num_words=vocab_size+1)#상위단어 5개만 사용\n",
        "tokenizer.fit_on_texts(preprocessed_sentence)\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentence)) #위의 두줄은 적용되지 않고 마지막 줄만 적용이 되어있다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0DvclBixEtu",
        "outputId": "9ecde435-f1f9-4ac4-ae2f-be6fcb6b0065"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#한번에 적용을 하는 방법\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentence)\n",
        "vocab_size=5\n",
        "words_frequency=[word for word, index in tokenizer.word_index.items() if index>=vocab_size+1]\n",
        "\n",
        "for word in words_frequency:\n",
        "  del tokenizer.word_index[word]\n",
        "  del tokenizer.word_counts[word]\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQtHHU3cy45I",
        "outputId": "7b537860-e5b8-4025-d3dc-f64ade9d9fe5"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#keras의 tokenizer은 기본적으로 단어 집합에 없는 OOV는 삭제를 해버린다.\n",
        "#이에 대한 보완으로 oov_token에 OOV를 추가하여 사용할 수 있다.\n",
        "#숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
        "vocab_size=5\n",
        "tokenizer=Tokenizer(num_words=vocab_size+2, oov_token='OOV')\n",
        "tokenizer.fit_on_texts(preprocessed_sentence)\n",
        "print('OOV의 인덱스 : ', tokenizer.word_index['OOV'])\n",
        "#기본적으로 keras tokenizer은 OOV의 인덱스를 1로 설정한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYFz8zLt0XN4",
        "outputId": "99f1abf4-4a87-4c43-9f81-e65919144424"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOV의 인덱스 :  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모두 확인이 끝나면 정수 인코딩\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXK242tL2R_I",
        "outputId": "17f4456f-713c-4421-f9ed-07c2dc991a84"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. 패딩(padding)"
      ],
      "metadata": {
        "id": "6X39tP5u4iDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Numpy로 패딩하기\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import stopwords\n",
        "\n",
        "ll='Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry\\'s conflict with Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people). The series was originally published in English by Bloomsbury in the United Kingdom and Scholastic Press in the United States. All versions around the world are printed by Grafica Veneta in Italy.[1] A series of many genres, including fantasy, drama, coming-of-age fiction, and the British school story (which includes elements of mystery, thriller, adventure, horror, and romance), the world of Harry Potter explores numerous themes and includes many cultural meanings and references.[2] According to Rowling, the main theme is death.[3] Other major themes in the series include prejudice, corruption, and madness.[4] Since the release of the first novel, Harry Potter and the Philosopher\\'s Stone, on 26 June 1997, the books have found immense popularity, positive reviews, and commercial success worldwide. They have attracted a wide adult audience as well as younger readers and are widely considered cornerstones of modern literature.[5][6] As of February 2023, the books have sold more than 600 million copies worldwide, making them the best-selling book series in history, and have been available in 85 languages.[7] The last four books consecutively set records as the fastest-selling books in history, with the final instalment selling roughly 2.7 million copies in the United Kingdom and 8.3 million copies in the United States within twenty-four hours of its release. The original seven books were adapted into an eight-part namesake film series by Warner Bros. Pictures. In 2016, the total value of the Harry Potter franchise was estimated at $25 billion,[8] making Harry Potter one of the highest-grossing media franchises of all time. Harry Potter and the Cursed Child is a play based on a story co-written by Rowling. The success of the books and films has allowed the Harry Potter franchise to expand with numerous derivative works, a travelling exhibition that premiered in Chicago in 2009, a studio tour in London that opened in 2012, a digital platform on which J. K. Rowling updates the series with new information and insight, and a pentalogy of spin-off films premiering in November 2016 with Fantastic Beasts and Where to Find Them, among many other developments. Themed attractions, collectively known as The Wizarding World of Harry Potter, have been built at several Universal Destinations & Experiences amusement parks around the world.'\n",
        "sentences=sent_tokenize(ll)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxmv5eLr4dib",
        "outputId": "7f5084b2-f394-4533-8fff-3f2fa82be53a"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling.', 'The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry.', \"The main story arc concerns Harry's conflict with Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people).\", 'The series was originally published in English by Bloomsbury in the United Kingdom and Scholastic Press in the United States.', 'All versions around the world are printed by Grafica Veneta in Italy.', '[1] A series of many genres, including fantasy, drama, coming-of-age fiction, and the British school story (which includes elements of mystery, thriller, adventure, horror, and romance), the world of Harry Potter explores numerous themes and includes many cultural meanings and references.', '[2] According to Rowling, the main theme is death.', '[3] Other major themes in the series include prejudice, corruption, and madness.', \"[4] Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, positive reviews, and commercial success worldwide.\", 'They have attracted a wide adult audience as well as younger readers and are widely considered cornerstones of modern literature.', '[5][6] As of February 2023, the books have sold more than 600 million copies worldwide, making them the best-selling book series in history, and have been available in 85 languages.', '[7] The last four books consecutively set records as the fastest-selling books in history, with the final instalment selling roughly 2.7 million copies in the United Kingdom and 8.3 million copies in the United States within twenty-four hours of its release.', 'The original seven books were adapted into an eight-part namesake film series by Warner Bros. Pictures.', 'In 2016, the total value of the Harry Potter franchise was estimated at $25 billion,[8] making Harry Potter one of the highest-grossing media franchises of all time.', 'Harry Potter and the Cursed Child is a play based on a story co-written by Rowling.', 'The success of the books and films has allowed the Harry Potter franchise to expand with numerous derivative works, a travelling exhibition that premiered in Chicago in 2009, a studio tour in London that opened in 2012, a digital platform on which J. K. Rowling updates the series with new information and insight, and a pentalogy of spin-off films premiering in November 2016 with Fantastic Beasts and Where to Find Them, among many other developments.', 'Themed attractions, collectively known as The Wizarding World of Harry Potter, have been built at several Universal Destinations & Experiences amusement parks around the world.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={}\n",
        "preprocessed_sentences=[]\n",
        "wordstop=set(stopwords.words('english'))\n",
        "for sentence in sentences:\n",
        "  result=[]\n",
        "  tokenized_sentence=word_tokenize(sentence.lower())\n",
        "  for word in tokenized_sentence:\n",
        "    if word not in wordstop:\n",
        "      if len(word)>2:\n",
        "        result.append(word)\n",
        "        if word not in vocab:\n",
        "          vocab[word]=0\n",
        "        vocab[word]+=1\n",
        "  preprocessed_sentences.append(result)\n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbLgBZ2f276x",
        "outputId": "920182d3-c83b-404d-bd8c-dab8a629bf22"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['harry', 'potter', 'series', 'seven', 'fantasy', 'novels', 'written', 'british', 'author', 'rowling'], ['novels', 'chronicle', 'lives', 'young', 'wizard', 'harry', 'potter', 'friends', 'hermione', 'granger', 'ron', 'weasley', 'students', 'hogwarts', 'school', 'witchcraft', 'wizardry'], ['main', 'story', 'arc', 'concerns', 'harry', 'conflict', 'lord', 'voldemort', 'dark', 'wizard', 'intends', 'become', 'immortal', 'overthrow', 'wizard', 'governing', 'body', 'known', 'ministry', 'magic', 'subjugate', 'wizards', 'muggles', 'non-magical', 'people'], ['series', 'originally', 'published', 'english', 'bloomsbury', 'united', 'kingdom', 'scholastic', 'press', 'united', 'states'], ['versions', 'around', 'world', 'printed', 'grafica', 'veneta', 'italy'], ['series', 'many', 'genres', 'including', 'fantasy', 'drama', 'coming-of-age', 'fiction', 'british', 'school', 'story', 'includes', 'elements', 'mystery', 'thriller', 'adventure', 'horror', 'romance', 'world', 'harry', 'potter', 'explores', 'numerous', 'themes', 'includes', 'many', 'cultural', 'meanings', 'references'], ['according', 'rowling', 'main', 'theme', 'death'], ['major', 'themes', 'series', 'include', 'prejudice', 'corruption', 'madness'], ['since', 'release', 'first', 'novel', 'harry', 'potter', 'philosopher', 'stone', 'june', '1997', 'books', 'found', 'immense', 'popularity', 'positive', 'reviews', 'commercial', 'success', 'worldwide'], ['attracted', 'wide', 'adult', 'audience', 'well', 'younger', 'readers', 'widely', 'considered', 'cornerstones', 'modern', 'literature'], ['february', '2023', 'books', 'sold', '600', 'million', 'copies', 'worldwide', 'making', 'best-selling', 'book', 'series', 'history', 'available', 'languages'], ['last', 'four', 'books', 'consecutively', 'set', 'records', 'fastest-selling', 'books', 'history', 'final', 'instalment', 'selling', 'roughly', '2.7', 'million', 'copies', 'united', 'kingdom', '8.3', 'million', 'copies', 'united', 'states', 'within', 'twenty-four', 'hours', 'release'], ['original', 'seven', 'books', 'adapted', 'eight-part', 'namesake', 'film', 'series', 'warner', 'bros.', 'pictures'], ['2016', 'total', 'value', 'harry', 'potter', 'franchise', 'estimated', 'billion', 'making', 'harry', 'potter', 'one', 'highest-grossing', 'media', 'franchises', 'time'], ['harry', 'potter', 'cursed', 'child', 'play', 'based', 'story', 'co-written', 'rowling'], ['success', 'books', 'films', 'allowed', 'harry', 'potter', 'franchise', 'expand', 'numerous', 'derivative', 'works', 'travelling', 'exhibition', 'premiered', 'chicago', '2009', 'studio', 'tour', 'london', 'opened', '2012', 'digital', 'platform', 'rowling', 'updates', 'series', 'new', 'information', 'insight', 'pentalogy', 'spin-off', 'films', 'premiering', 'november', '2016', 'fantastic', 'beasts', 'find', 'among', 'many', 'developments'], ['themed', 'attractions', 'collectively', 'known', 'wizarding', 'world', 'harry', 'potter', 'built', 'several', 'universal', 'destinations', 'experiences', 'amusement', 'parks', 'around', 'world']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)\n",
        "print(vocab['harry'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7P_cnLBygqr",
        "outputId": "7b7e148a-4e94-4e3a-87bb-71e4cc8e29dc"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'harry': 10, 'potter': 9, 'series': 7, 'seven': 2, 'fantasy': 2, 'novels': 2, 'written': 1, 'british': 2, 'author': 1, 'rowling': 4, 'chronicle': 1, 'lives': 1, 'young': 1, 'wizard': 3, 'friends': 1, 'hermione': 1, 'granger': 1, 'ron': 1, 'weasley': 1, 'students': 1, 'hogwarts': 1, 'school': 2, 'witchcraft': 1, 'wizardry': 1, 'main': 2, 'story': 3, 'arc': 1, 'concerns': 1, 'conflict': 1, 'lord': 1, 'voldemort': 1, 'dark': 1, 'intends': 1, 'become': 1, 'immortal': 1, 'overthrow': 1, 'governing': 1, 'body': 1, 'known': 2, 'ministry': 1, 'magic': 1, 'subjugate': 1, 'wizards': 1, 'muggles': 1, 'non-magical': 1, 'people': 1, 'originally': 1, 'published': 1, 'english': 1, 'bloomsbury': 1, 'united': 4, 'kingdom': 2, 'scholastic': 1, 'press': 1, 'states': 2, 'versions': 1, 'around': 2, 'world': 4, 'printed': 1, 'grafica': 1, 'veneta': 1, 'italy': 1, 'many': 3, 'genres': 1, 'including': 1, 'drama': 1, 'coming-of-age': 1, 'fiction': 1, 'includes': 2, 'elements': 1, 'mystery': 1, 'thriller': 1, 'adventure': 1, 'horror': 1, 'romance': 1, 'explores': 1, 'numerous': 2, 'themes': 2, 'cultural': 1, 'meanings': 1, 'references': 1, 'according': 1, 'theme': 1, 'death': 1, 'major': 1, 'include': 1, 'prejudice': 1, 'corruption': 1, 'madness': 1, 'since': 1, 'release': 2, 'first': 1, 'novel': 1, 'philosopher': 1, 'stone': 1, 'june': 1, '1997': 1, 'books': 6, 'found': 1, 'immense': 1, 'popularity': 1, 'positive': 1, 'reviews': 1, 'commercial': 1, 'success': 2, 'worldwide': 2, 'attracted': 1, 'wide': 1, 'adult': 1, 'audience': 1, 'well': 1, 'younger': 1, 'readers': 1, 'widely': 1, 'considered': 1, 'cornerstones': 1, 'modern': 1, 'literature': 1, 'february': 1, '2023': 1, 'sold': 1, '600': 1, 'million': 3, 'copies': 3, 'making': 2, 'best-selling': 1, 'book': 1, 'history': 2, 'available': 1, 'languages': 1, 'last': 1, 'four': 1, 'consecutively': 1, 'set': 1, 'records': 1, 'fastest-selling': 1, 'final': 1, 'instalment': 1, 'selling': 1, 'roughly': 1, '2.7': 1, '8.3': 1, 'within': 1, 'twenty-four': 1, 'hours': 1, 'original': 1, 'adapted': 1, 'eight-part': 1, 'namesake': 1, 'film': 1, 'warner': 1, 'bros.': 1, 'pictures': 1, '2016': 2, 'total': 1, 'value': 1, 'franchise': 2, 'estimated': 1, 'billion': 1, 'one': 1, 'highest-grossing': 1, 'media': 1, 'franchises': 1, 'time': 1, 'cursed': 1, 'child': 1, 'play': 1, 'based': 1, 'co-written': 1, 'films': 2, 'allowed': 1, 'expand': 1, 'derivative': 1, 'works': 1, 'travelling': 1, 'exhibition': 1, 'premiered': 1, 'chicago': 1, '2009': 1, 'studio': 1, 'tour': 1, 'london': 1, 'opened': 1, '2012': 1, 'digital': 1, 'platform': 1, 'updates': 1, 'new': 1, 'information': 1, 'insight': 1, 'pentalogy': 1, 'spin-off': 1, 'premiering': 1, 'november': 1, 'fantastic': 1, 'beasts': 1, 'find': 1, 'among': 1, 'developments': 1, 'themed': 1, 'attractions': 1, 'collectively': 1, 'wizarding': 1, 'built': 1, 'several': 1, 'universal': 1, 'destinations': 1, 'experiences': 1, 'amusement': 1, 'parks': 1}\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sorted=sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
        "print(vocab_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCYncvnTBR5w",
        "outputId": "263b3d79-4468-4858-b6dc-66db3b6605ed"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('harry', 10), ('potter', 9), ('series', 7), ('books', 6), ('rowling', 4), ('united', 4), ('world', 4), ('wizard', 3), ('story', 3), ('many', 3), ('million', 3), ('copies', 3), ('seven', 2), ('fantasy', 2), ('novels', 2), ('british', 2), ('school', 2), ('main', 2), ('known', 2), ('kingdom', 2), ('states', 2), ('around', 2), ('includes', 2), ('numerous', 2), ('themes', 2), ('release', 2), ('success', 2), ('worldwide', 2), ('making', 2), ('history', 2), ('2016', 2), ('franchise', 2), ('films', 2), ('written', 1), ('author', 1), ('chronicle', 1), ('lives', 1), ('young', 1), ('friends', 1), ('hermione', 1), ('granger', 1), ('ron', 1), ('weasley', 1), ('students', 1), ('hogwarts', 1), ('witchcraft', 1), ('wizardry', 1), ('arc', 1), ('concerns', 1), ('conflict', 1), ('lord', 1), ('voldemort', 1), ('dark', 1), ('intends', 1), ('become', 1), ('immortal', 1), ('overthrow', 1), ('governing', 1), ('body', 1), ('ministry', 1), ('magic', 1), ('subjugate', 1), ('wizards', 1), ('muggles', 1), ('non-magical', 1), ('people', 1), ('originally', 1), ('published', 1), ('english', 1), ('bloomsbury', 1), ('scholastic', 1), ('press', 1), ('versions', 1), ('printed', 1), ('grafica', 1), ('veneta', 1), ('italy', 1), ('genres', 1), ('including', 1), ('drama', 1), ('coming-of-age', 1), ('fiction', 1), ('elements', 1), ('mystery', 1), ('thriller', 1), ('adventure', 1), ('horror', 1), ('romance', 1), ('explores', 1), ('cultural', 1), ('meanings', 1), ('references', 1), ('according', 1), ('theme', 1), ('death', 1), ('major', 1), ('include', 1), ('prejudice', 1), ('corruption', 1), ('madness', 1), ('since', 1), ('first', 1), ('novel', 1), ('philosopher', 1), ('stone', 1), ('june', 1), ('1997', 1), ('found', 1), ('immense', 1), ('popularity', 1), ('positive', 1), ('reviews', 1), ('commercial', 1), ('attracted', 1), ('wide', 1), ('adult', 1), ('audience', 1), ('well', 1), ('younger', 1), ('readers', 1), ('widely', 1), ('considered', 1), ('cornerstones', 1), ('modern', 1), ('literature', 1), ('february', 1), ('2023', 1), ('sold', 1), ('600', 1), ('best-selling', 1), ('book', 1), ('available', 1), ('languages', 1), ('last', 1), ('four', 1), ('consecutively', 1), ('set', 1), ('records', 1), ('fastest-selling', 1), ('final', 1), ('instalment', 1), ('selling', 1), ('roughly', 1), ('2.7', 1), ('8.3', 1), ('within', 1), ('twenty-four', 1), ('hours', 1), ('original', 1), ('adapted', 1), ('eight-part', 1), ('namesake', 1), ('film', 1), ('warner', 1), ('bros.', 1), ('pictures', 1), ('total', 1), ('value', 1), ('estimated', 1), ('billion', 1), ('one', 1), ('highest-grossing', 1), ('media', 1), ('franchises', 1), ('time', 1), ('cursed', 1), ('child', 1), ('play', 1), ('based', 1), ('co-written', 1), ('allowed', 1), ('expand', 1), ('derivative', 1), ('works', 1), ('travelling', 1), ('exhibition', 1), ('premiered', 1), ('chicago', 1), ('2009', 1), ('studio', 1), ('tour', 1), ('london', 1), ('opened', 1), ('2012', 1), ('digital', 1), ('platform', 1), ('updates', 1), ('new', 1), ('information', 1), ('insight', 1), ('pentalogy', 1), ('spin-off', 1), ('premiering', 1), ('november', 1), ('fantastic', 1), ('beasts', 1), ('find', 1), ('among', 1), ('developments', 1), ('themed', 1), ('attractions', 1), ('collectively', 1), ('wizarding', 1), ('built', 1), ('several', 1), ('universal', 1), ('destinations', 1), ('experiences', 1), ('amusement', 1), ('parks', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={}\n",
        "i=1\n",
        "for (word, frequency) in vocab_sorted:\n",
        "  if frequency >1 :\n",
        "    word_to_index[word]=i\n",
        "    i+=1\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgsZDYsYBqLe",
        "outputId": "0ca36b95-307b-475b-8c61-b9778de39d89"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'harry': 1, 'potter': 2, 'series': 3, 'books': 4, 'rowling': 5, 'united': 6, 'world': 7, 'wizard': 8, 'story': 9, 'many': 10, 'million': 11, 'copies': 12, 'seven': 13, 'fantasy': 14, 'novels': 15, 'british': 16, 'school': 17, 'main': 18, 'known': 19, 'kingdom': 20, 'states': 21, 'around': 22, 'includes': 23, 'numerous': 24, 'themes': 25, 'release': 26, 'success': 27, 'worldwide': 28, 'making': 29, 'history': 30, '2016': 31, 'franchise': 32, 'films': 33}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=10\n",
        "word_frequency=[word for word, index in word_to_index.items() if index>vocab_size]\n",
        "for word in word_frequency:\n",
        "  del word_to_index[word]\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZTfFjX6CajP",
        "outputId": "b74cff03-2f96-4296-bfb3-865d3b7c1a8d"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'harry': 1, 'potter': 2, 'series': 3, 'books': 4, 'rowling': 5, 'united': 6, 'world': 7, 'wizard': 8, 'story': 9, 'many': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index['OOV']=len(word_to_index)+1"
      ],
      "metadata": {
        "id": "r8voNcdaEHq8"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sentences=[]\n",
        "for sentence in preprocessed_sentences:\n",
        "  encoded_sentence=[]\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      encoded_sentence.append(word_to_index[word])\n",
        "    except KeyError:\n",
        "      encoded_sentence.append(word_to_index['OOV'])\n",
        "  encoded_sentences.append(encoded_sentence)\n",
        "print(encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr2ahq5lDQv4",
        "outputId": "366a326b-b1c1-413e-9dc4-6a9acf99f426"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 11, 11, 11, 11, 11, 11, 5], [11, 11, 11, 11, 8, 1, 2, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11], [11, 9, 11, 11, 1, 11, 11, 11, 11, 8, 11, 11, 11, 11, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11], [3, 11, 11, 11, 11, 6, 11, 11, 11, 6, 11], [11, 11, 7, 11, 11, 11, 11], [3, 10, 11, 11, 11, 11, 11, 11, 11, 11, 9, 11, 11, 11, 11, 11, 11, 11, 7, 1, 2, 11, 11, 11, 11, 10, 11, 11, 11], [11, 5, 11, 11, 11], [11, 11, 3, 11, 11, 11, 11], [11, 11, 11, 11, 1, 2, 11, 11, 11, 11, 4, 11, 11, 11, 11, 11, 11, 11, 11], [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11], [11, 11, 4, 11, 11, 11, 11, 11, 11, 11, 11, 3, 11, 11, 11], [11, 11, 4, 11, 11, 11, 11, 4, 11, 11, 11, 11, 11, 11, 11, 11, 6, 11, 11, 11, 11, 6, 11, 11, 11, 11, 11], [11, 11, 4, 11, 11, 11, 11, 3, 11, 11, 11], [11, 11, 11, 1, 2, 11, 11, 11, 11, 1, 2, 11, 11, 11, 11, 11], [1, 2, 11, 11, 11, 11, 9, 11, 5], [11, 4, 11, 11, 1, 2, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 5, 11, 3, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 11], [11, 11, 11, 11, 11, 7, 1, 2, 11, 11, 11, 11, 11, 11, 11, 11, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##여기부터 패딩\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "encoded=tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY2LAE2lFAwH",
        "outputId": "30ca9627-9bf3-4e5a-bd4e-9f99d5d5f35f"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 13, 14, 15, 34, 16, 35, 5], [15, 36, 37, 38, 8, 1, 2, 39, 40, 41, 42, 43, 44, 45, 17, 46, 47], [18, 9, 48, 49, 1, 50, 51, 52, 53, 8, 54, 55, 56, 57, 8, 58, 59, 19, 60, 61, 62, 63, 64, 65, 66], [3, 67, 68, 69, 70, 6, 20, 71, 72, 6, 21], [73, 22, 7, 74, 75, 76, 77], [3, 10, 78, 79, 14, 80, 81, 82, 16, 17, 9, 23, 83, 84, 85, 86, 87, 88, 7, 1, 2, 89, 24, 25, 23, 10, 90, 91, 92], [93, 5, 18, 94, 95], [96, 25, 3, 97, 98, 99, 100], [101, 26, 102, 103, 1, 2, 104, 105, 106, 107, 4, 108, 109, 110, 111, 112, 113, 27, 28], [114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125], [126, 127, 4, 128, 129, 11, 12, 28, 29, 130, 131, 3, 30, 132, 133], [134, 135, 4, 136, 137, 138, 139, 4, 30, 140, 141, 142, 143, 144, 11, 12, 6, 20, 145, 11, 12, 6, 21, 146, 147, 148, 26], [149, 13, 4, 150, 151, 152, 153, 3, 154, 155, 156], [31, 157, 158, 1, 2, 32, 159, 160, 29, 1, 2, 161, 162, 163, 164, 165], [1, 2, 166, 167, 168, 169, 9, 170, 5], [27, 4, 33, 171, 1, 2, 32, 172, 24, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 5, 187, 3, 188, 189, 190, 191, 192, 33, 193, 194, 31, 195, 196, 197, 198, 10, 199], [200, 201, 202, 19, 203, 7, 1, 2, 204, 205, 206, 207, 208, 209, 210, 22, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=max(len(item) for item in encoded)\n",
        "#encoded된 문장 중에 가장 긴 문장의 길이는 41이다\n",
        "print('최대 길이 : ', max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjKSm6_KFfRw",
        "outputId": "f4c37314-13f0-4345-faa3-340705535acc"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최대 길이 :  41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#최대 길이에 맞추어 패딩\n",
        "for sentence in encoded:\n",
        "  while len(sentence) < max_len:\n",
        "    sentence.append(0)\n",
        "padded_up=np.array(encoded)\n",
        "print(padded_up[:10])\n",
        "#숫자 0을 사용한 제로 패딩"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PJw1xLnFsN5",
        "outputId": "55c579fd-4b20-42f6-900a-7243a6636ee6"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1   2   3  13  14  15  34  16  35   5   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [ 15  36  37  38   8   1   2  39  40  41  42  43  44  45  17  46  47   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [ 18   9  48  49   1  50  51  52  53   8  54  55  56  57   8  58  59  19\n",
            "   60  61  62  63  64  65  66   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [  3  67  68  69  70   6  20  71  72   6  21   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [ 73  22   7  74  75  76  77   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [  3  10  78  79  14  80  81  82  16  17   9  23  83  84  85  86  87  88\n",
            "    7   1   2  89  24  25  23  10  90  91  92   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [ 93   5  18  94  95   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [ 96  25   3  97  98  99 100   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [101  26 102 103   1   2 104 105 106 107   4 108 109 110 111 112 113  27\n",
            "   28   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [114 115 116 117 118 119 120 121 122 123 124 125   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#케라스의 전처리 도구 패딩\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "encoded=tokenizer.texts_to_sequences(preprocessed_sentences)\n"
      ],
      "metadata": {
        "id": "VK6W5fEqGCWQ"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded=pad_sequences(encoded)\n",
        "print(padded[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9zAFfEpG5O5",
        "outputId": "7b10a104-f2bb-44ad-8eac-032055ec62d1"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   1   2   3  13  14\n",
            "   15  34  16  35   5]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0  15  36  37  38   8   1   2  39  40  41  42  43\n",
            "   44  45  17  46  47]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  18   9\n",
            "   48  49   1  50  51  52  53   8  54  55  56  57   8  58  59  19  60  61\n",
            "   62  63  64  65  66]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   3  67  68  69  70   6\n",
            "   20  71  72   6  21]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  73  22\n",
            "    7  74  75  76  77]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  10  78  79  14  80\n",
            "   81  82  16  17   9  23  83  84  85  86  87  88   7   1   2  89  24  25\n",
            "   23  10  90  91  92]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   93   5  18  94  95]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  96  25\n",
            "    3  97  98  99 100]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0 101  26 102 103   1   2 104 105 106 107   4 108 109 110\n",
            "  111 112 113  27  28]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0 114 115 116 117 118 119 120\n",
            "  121 122 123 124 125]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded=pad_sequences(encoded, padding='post', maxlen=17)\n",
        "#최대길이를 17로 설정하고 post를 넣으며 뒤에 0이 붙는다.\n",
        "#데이터가 maxlen때문에 손실될 경우 truncationg='post'를 이용하여 뒤에서부터 데이터가 삭제된다.\n",
        "#value=len(tokenizer.word_index)+1을 사용하여 길이보다 1이 큰 수로 0대신 패딩을 할 수 있다.\n",
        "print(padded[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrrGOStaG_9I",
        "outputId": "e6ee61cc-116b-4788-b506-7acfa80af892"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1   2   3  13  14  15  34  16  35   5   0   0   0   0   0   0   0]\n",
            " [ 15  36  37  38   8   1   2  39  40  41  42  43  44  45  17  46  47]\n",
            " [ 53   8  54  55  56  57   8  58  59  19  60  61  62  63  64  65  66]\n",
            " [  3  67  68  69  70   6  20  71  72   6  21   0   0   0   0   0   0]\n",
            " [ 73  22   7  74  75  76  77   0   0   0   0   0   0   0   0   0   0]\n",
            " [ 83  84  85  86  87  88   7   1   2  89  24  25  23  10  90  91  92]\n",
            " [ 93   5  18  94  95   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [ 96  25   3  97  98  99 100   0   0   0   0   0   0   0   0   0   0]\n",
            " [102 103   1   2 104 105 106 107   4 108 109 110 111 112 113  27  28]\n",
            " [114 115 116 117 118 119 120 121 122 123 124 125   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZYgHFftHGdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}