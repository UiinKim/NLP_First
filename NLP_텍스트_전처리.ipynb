{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIrtCqryrFB0lNp/8OIHdq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UiinKim/UiinKim/blob/main/NLP_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  1. 토큰화 -> 토큰으로 나누는 기준마다 다르다(단어, 문장 등)"
      ],
      "metadata": {
        "id": "KxQXxKrW5l2L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "wzHKPh7cWuP0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3slP54Q9plPB",
        "outputId": "32870214-6d9f-4a3c-8e00-099766d7c9c1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 토큰화에서는 단순 띄어쓰기, 어퍼스트로피, 특수문자로 구분해서는 안된다.\n",
        "sentence=\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
        "print(word_tokenize(sentence))\n",
        "print(WordPunctTokenizer().tokenize(sentence))\n",
        "print(text_to_word_sequence(sentence))\n",
        "print(TreebankWordTokenizer().tokenize(sentence))\n",
        "#토큰화 도구마다 말뭉치(코퍼스)가 다르다\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_H4kbuuW4hd",
        "outputId": "99bfee2f-610b-4098-89e3-951892486dfb"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문장 토큰화에서는 '.'으로 구분하면 ip주소나 이메일에서 걸린다.\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print(sent_tokenize(text))\n",
        "#밑에는 마침표가 여러개 등장하는 경우\n",
        "text_cf=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print(sent_tokenize(text_cf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ditc-tzvpRNr",
        "outputId": "b0d6a303-ca07-4643-e247-c97b501ed15e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n",
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss\n",
        "#kss는 한국어 문장 토큰화 도구"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvTiFNjMryi7",
        "outputId": "a11fd818-38fd-4974-f0cf-d9583589e33c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kss in /usr/local/lib/python3.10/dist-packages (4.5.3)\n",
            "Requirement already satisfied: emoji==1.2.0 in /usr/local/lib/python3.10/dist-packages (from kss) (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from kss) (2022.10.31)\n",
            "Requirement already satisfied: pecab in /usr/local/lib/python3.10/dist-packages (from kss) (1.0.8)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (1.22.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (9.0.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (7.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.1.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB9nwQEgucu8",
        "outputId": "07cbbeef-a1b1-463a-8d20-84dcc0d4a4be"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "\n",
        "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))\n",
        "#한국어는 영어와 달리 어절 토큰화가 아닌 형태소 토큰화를 시켜야한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EItIkL6bsR3b",
        "outputId": "54090b1b-2767-403a-e31f-0993b65ff5df"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "품사태깅 한국어의 '못', 영어의 'fly' -> 의미를 명확하게 알기 위해서 사용"
      ],
      "metadata": {
        "id": "IYmscEVq5h8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "tokenized_sentence=word_tokenize(text)\n",
        "print('단어 토큰화 : {}'.format(tokenized_sentence))\n",
        "print('품사 태깅 : {}'.format(pos_tag(tokenized_sentence)))\n",
        "#PRP는 인칭대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유명사, NNS는 복수형 명사, CC는 접속사, DT는 관사"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k9pkn_WseNz",
        "outputId": "efedc738-59a4-4ef1-ba47-f2f759140953"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFQoxsUgvBsE",
        "outputId": "040321f7-9cfc-43c3-d45c-1b1695896025"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "okt=Okt()\n",
        "kkma=Kkma()\n",
        "\n",
        "text=\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "\n",
        "print('OKT 형태소 분석 : {}'.format(okt.morphs(text)))\n",
        "print('Kkma 형태소 분석 : {}'.format(kkma.morphs(text)))\n",
        "print('OKT 품사 태깅 : {}'.format(okt.pos(text)))\n",
        "print('Kkma 품사 태깅 : {}'.format(kkma.pos(text)))\n",
        "print('OKT 명사 추출 : {}'.format(okt.nouns(text)))\n",
        "print('Kkma 명사 추출 : {}'.format(kkma.nouns(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcwVOD6ytqrb",
        "outputId": "9bdbf8af-c997-4587-d10d-7adda44b5b2d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
            "Kkma 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
            "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
            "Kkma 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
            "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n",
            "Kkma 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2.정제(코퍼스에서 노이즈 데이터--> 등장 빈도가 낮은 단어, 길이가 짧은 단어 제거) 및 정규화(표현 방법이 다른 단어들을 통합)"
      ],
      "metadata": {
        "id": "spT0Ji0l5e2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re #regular expression 정규표현식\n",
        "text=\"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "\n",
        "#단어의 길이 1~2인 단어를 정규표현식으로 삭제\n",
        "shortword=re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "#compile 정규식 개체 리턴\n",
        "print(shortword.sub('',text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLbIp8G3vAM8",
        "outputId": "42b7ac1a-7820-48f3-d560-9929018c2503"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6f-M1Ys0St6",
        "outputId": "bf394d6b-64ff-4784-a591-3c8444e79a82"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 정규화 기법 : 어간 추출, 표제어(사전에 있는 단어) 추출 --> 하나의 단어로 일반화 시킬 수 있으면 문서 내의 단어의 수를 줄이겠다 -> BoW(Bag of Word)에서 사용\n",
        "\n",
        " 표제어 추출"
      ],
      "metadata": {
        "id": "cpdNIr8M5YEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#표제어(Lemma) : are, is, am -> be\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "\n",
        "print('표제어 추출 전 : {}'.format(words))\n",
        "print('표제어 추출 후 : {}'.format([lemmatizer.lemmatize(word) for word in words]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US312299yo_E",
        "outputId": "3174a931-9181-421d-87d1-779f673c8ad6"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WordNetLemmatizer은 품사를 알려주면 품사의 정보를 보존하며 정확학 Lemma가 추출된다.\n",
        "print(lemmatizer.lemmatize('dies', 'v'))\n",
        "print(lemmatizer.lemmatize('has', 'v'))\n",
        "print(lemmatizer.lemmatize('watched', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1zfpU8h0EpW",
        "outputId": "f368f666-5509-44ad-9018-9f8b57cc915d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "die\n",
            "have\n",
            "watch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어간(Stem) 추출"
      ],
      "metadata": {
        "id": "OyL8VFlD5Shy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer=PorterStemmer()\n",
        "\n",
        "sentence=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "tokenized_sentence=word_tokenize(sentence)\n",
        "print('어간 추출 전 : {}'.format(tokenized_sentence))\n",
        "print('어간 추출 후 : {}'.format([stemmer.stem(word) for word in tokenized_sentence]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4qdFg7507td",
        "outputId": "615cb236-91f3-4302-f884-65f0b67d5cda"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
            "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer=PorterStemmer()\n",
        "lancaster_stemmer=LancasterStemmer()\n",
        "\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print('포터스태머 : ', [porter_stemmer.stem(word) for word in words])\n",
        "print('랭커스태머 : ', [lancaster_stemmer.stem(word) for word in words])\n",
        "#알고리즘이 다르기 때문에 매우 다른 결과가 나온다. 표제어 추출과도 매우 다르게 나온다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJMZgg7414qG",
        "outputId": "bc987929-61ed-41a4-dde8-06654933ad01"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "포터스태머 :  ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
            "랭커스태머 :  ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 불용어 : 자주 등장하지만 분석을 하는데에 불필요한 단어\n"
      ],
      "metadata": {
        "id": "d2dpA_WA5Lpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "lr1Nl1fO5K4f"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpeeOYL06YR0",
        "outputId": "d240ca78-5599-487d-d73f-e7d1307f4a33"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_word_list=stopwords.words('english')\n",
        "print('불용어의 개수 : {}'.format(len(stop_word_list)))\n",
        "print('영어의 불용어 10개 출력 : {}'.format(stop_word_list[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuvY202F5NA_",
        "outputId": "fbeb1d49-3536-4548-d22a-331d0a87d457"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어의 개수 : 179\n",
            "영어의 불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK를 통해 불용어 제거하기\n",
        "example=\"Family is not an important thing. It's everything.\"\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "tokenized_sentence=word_tokenize(example)\n",
        "\n",
        "result=[]\n",
        "for word in tokenized_sentence:\n",
        "  if word not in stop_words:\n",
        "    result.append(word)\n",
        "\n",
        "print(\"불용어 제거 전 : {}\".format(tokenized_sentence))\n",
        "print('불용어 제거 후 : {}'.format(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5hTa_YL6W1Q",
        "outputId": "6c980f93-29b1-4e40-e893-249e5aa13927"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#한국어 불용어 제거하기(절대적인 기준이 아니므로 사용자 설정을 하는 경우가 많음)\n",
        "okt=Okt()\n",
        "example=\"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words=\"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "\n",
        "stop_words=set(stop_words.split(' '))\n",
        "word_tokens=okt.morphs(example)\n",
        "\n",
        "result=[word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print('불용어 제거 전 : {}'.format(word_tokens))\n",
        "print('불용어 제거 후 : {}'.format(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnsTcB6g7PAA",
        "outputId": "0310c9e5-2d18-42b8-d18f-ff4402e72778"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 정규표현식 -> re와 nltk를 통해 토큰화하는데 유용\n",
        "\n"
      ],
      "metadata": {
        "id": "tT7oRjUP8atd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "r=re.compile('a.c')\n",
        "r.search('kkk')\n",
        "#아무것도 출력되지 않는다."
      ],
      "metadata": {
        "id": "xhIQapPT8NM4"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search('abc')\n",
        "# '.'은 한 개의 임의의 문자를 나타낸다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSWgR2Sm9od7",
        "outputId": "d0b789f5-fd0f-4d95-b71c-e44d5aea65f3"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r=re.compile('ab?c')\n",
        "r.search('abbbc')\n",
        "#아무것도 출력되지 않는다."
      ],
      "metadata": {
        "id": "2rTZIdEy9MHg"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.search('abc')\n",
        "# '?'앞의 문자인 b가 존재할 수도 있고 존재하지 않을 수도 있다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaHzX55S9Nrw",
        "outputId": "f9b468d5-d667-4a5c-dd28-923a171f15ad"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.search('ac')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q7aFbDs9rkX",
        "outputId": "bfa3e2de-981b-4900-97f0-23cc8203b353"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ac'>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# . : 대체할 수 있다.\n",
        "# ? : ? 앞의 문자가 존재할 수도 있고 존재하지 않을 수도 있다.\n",
        "# * : * 앞의 문자가 0개 이상이다.\n",
        "# + : + 앞의 문자가 1개 이상이다.\n",
        "# ^ : ^ 뒤의 문자열이 모두 포함하여 시작되는 경우 매치한다.\n",
        "# {숫자} : {숫자} 앞의 문자를 숫자만큼 반복하여 나타거나 매치한다.\n",
        "# {숫자1, 숫자2} : {숫자 1, 숫자 2} 앞의 문자를 숫자 1과 숫자 2 사이만큼 반복하여 나타내거나 매치한다.\n",
        "# {숫자, } : {숫자} 앞의 문자를 숫자 이상만큼 반복한다\n",
        "# [] : [] 안의 문자들 중 한개의 문자와 매치한다. [A-Za-z]는 모든 알파벳을 의미하고 [0-9]는 숫자 전부를 의미한다.\n",
        "# [^문자] : [^문자]를 제외한 모든 문자를 매치한다. [^abc]이면 a or b or c를 제외한 모든 문자\n"
      ],
      "metadata": {
        "id": "0jb87yhB95Nf"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 정수 인코딩(정수와 텍스트를 맵핑)\n",
        "6-1. dictionary 사용"
      ],
      "metadata": {
        "id": "h7jnnLJUc3Ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "2zbrWztG9r_d"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text=\"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "metadata": {
        "id": "z4-nsFm9dd4Z"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#문장 토큰화\n",
        "sentences=sent_tokenize(raw_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7kE5IMpdkKo",
        "outputId": "1be087e5-377b-4891-d212-8b9e6448144c"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 정제(불용어)\n",
        "vocab={}\n",
        "preprocessed_sentence=[]\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "  #단어 토큰화\n",
        "  tokenized_sentence=word_tokenize(sentence)\n",
        "  result=[]\n",
        "\n",
        "  for word in tokenized_sentence:\n",
        "    word=word.lower() #모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
        "    if word not in stop_words: #단어 토큰화 된 결과에 대해 불용어를 제거한다.\n",
        "      if len(word) >2: #단어의 길이가 2 이하인 경우 추가로 단어를 제거한다.\n",
        "        result.append(word)\n",
        "        if word not in vocab:\n",
        "          vocab[word]=0 #처음 등장인 경우 해당 단어를 0으로 초기화\n",
        "        vocab[word]+=1 #등장한 횟수만큼 +1이 된다.\n",
        "  preprocessed_sentence.append(result) #불용어를 제거한 단어들만 추가한다.\n",
        "print(preprocessed_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHddX14sd6TI",
        "outputId": "d0949896-a638-431b-cdf6-3797100ab1d6"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 집합 : ', vocab)\n",
        "print('barber의 빈도 수 : ', vocab['barber'])\n",
        "#빈도수 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAuNi_TQgPRR",
        "outputId": "5327e57b-6fc8-4400-b05d-103d59915cda"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 :  {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
            "barber의 빈도 수 :  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#빈도수대로 정렬\n",
        "vocab_sorted=sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
        "# vocab.items()는 vocab의 key와 value를 모두 포함한다는 뜻이고 lambda는 새로운 x로 설정하고 싶은 값을 x:원하는 값으로 설정한다.\n",
        "print(vocab_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqrPmKMNggdp",
        "outputId": "e4d60e1b-0bcb-4f19-ba66-642af469764d"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={}\n",
        "i=0 # 높은 빈도수를 가진 단어부터 1부터 입력\n",
        "for(word, frequency) in vocab_sorted:\n",
        "  if frequency > 1: #빈도수가 1회 이하이면 제외한다.\n",
        "    i=i+1\n",
        "    word_to_index[word]=i\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZELzNd5g3Vp",
        "outputId": "9351edb0-6071-4937-ec37-338845e858c9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=5\n",
        "words_frequency=[word for word,index in word_to_index.items() if index>=vocab_size+1]\n",
        "#인덱스가 5보다 높은 단어 들만 리스트에 저장\n",
        "for word in words_frequency:\n",
        "  del word_to_index[word] #word_frequency에 포함된 단어 제거\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9ks2XMTka9F",
        "outputId": "e0c174b8-b5e6-4f44-8837-a624c612ed21"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어들을 정수로 인코딩\n",
        "#단어 집합에 존재하지 않는 단어들을 Out-Of_Vocabulary(OOV)라고 한다.\n",
        "word_to_index['OOV']=len(word_to_index)+1 #OOV의 단어의 경우를 추가해준다.\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojvn7xpflqlr",
        "outputId": "7717c697-71ff-46d1-d9c3-b6cc3be22320"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word_to_index를 사용하여 sentences의 모든 단어들을 맵핑되는 정수로 인코딩\n",
        "encoded_sentences=[]\n",
        "for sentence in preprocessed_sentence:\n",
        "  encoded_sentence=[]\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      #단어 집합에 있는 단어이면 해당 단어의 정수를 리턴.\n",
        "      encoded_sentence.append(word_to_index[word])\n",
        "    except KeyError:\n",
        "      #없는 경우 'OOV'를 리턴\n",
        "      encoded_sentence.append(word_to_index['OOV'])\n",
        "  encoded_sentences.append(encoded_sentence)\n",
        "print(encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nopBKAm_mQ-z",
        "outputId": "a81227da-7917-4c9d-ca6b-9d1bc73de434"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Counter 사용하기"
      ],
      "metadata": {
        "id": "nETOpmotnrmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter #위의 dictionary로 일일이 저장하는 방법을 api로 불러와 간단하게 하는 방법\n",
        "print(preprocessed_sentence)\n",
        "#prerprocessed_sentence는 단어 토큰화의 결과만 저장되어 있어 단어 집합을 만들기 위해 ','를 제거하고 단어들을 하나의 리스트로 만든다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKFe1Ur4nO2T",
        "outputId": "2f05feb1-2fa5-4e13-9fb8-c466a116054b"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words_list=sum(preprocessed_sentence, [])\n",
        "print(all_words_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q7sNXC1oTys",
        "outputId": "2fcb0b1a-334c-44db-defb-214410e6b951"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=Counter(all_words_list) #자동으로 빈도수에 따른 dictionary화 및 정렬\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0_7VM1LpcJc",
        "outputId": "59fb62d6-6178-4dc1-f90e-d4a39dff4f14"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=5\n",
        "vocab=vocab.most_common(vocab_size) #vocab_size만큼 가장 빈도수가 많은 것만 저장\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMA0nGa_pj0r",
        "outputId": "fdfbd8d8-d170-41b8-916e-188ac07745fd"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={}\n",
        "i=0\n",
        "for(word, frequency) in vocab:\n",
        "  i=i+1\n",
        "  word_to_index[word]=i\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFfCumU4pr1D",
        "outputId": "2a4a7d78-acf0-4fcd-fb93-ad8a7ead85b8"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6-3. NLTK의 FreqDist 사용하기"
      ],
      "metadata": {
        "id": "KnJPew8uq4lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jn13e2aKqHHN"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hstack은 행이 같으면 모든 리스트들을 column옆으로 쭉 추가한다. -> 문장 구분을 제거한다.\n",
        "vocab=FreqDist(np.hstack(preprocessed_sentence)) #Counter와 같은 방법이다."
      ],
      "metadata": {
        "id": "gxo2Vcwtq_51"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab['barber'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dxFKzMzrQaj",
        "outputId": "904e2f94-2de4-4b1e-bdd0-3e135a1f8770"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=5\n",
        "vocab=vocab.most_common(vocab_size)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF0Rll9erSvU",
        "outputId": "5e069d19-f184-4cb6-db88-2ca9cfb753e3"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppQrJOPttVDW"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#enumerate는 (인덱스, value)로 변환된다. 여기서 인덱스를 자동으로 구해주는 함수이다.\n",
        "word_to_index={word[0]:index+1 for index, word in enumerate(vocab)}\n",
        "#word는 vocab의 1튜플씩 나오며 word[0]은 (단어, 빈도수) 중에 단어에 해당한다. index는 enumerate를 통해 인덱스값이 추출 되며 +1을 하여 정수로 지정한다.\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRQz-a3Srf8M",
        "outputId": "1234a3a3-32d4-4c94-9d80-652fdf868a70"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#enumerate 실습\n",
        "#enumerate는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 리턴\n",
        "test_input=['a', 'b', 'c', 'd', 'e']\n",
        "for index, value in enumerate(test_input):\n",
        "  print(\"value : {}, index : {}\".format(value, index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0AUgfh1tfU1",
        "outputId": "7ec94e66-3deb-4da9-efdc-526eb0aaac76"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value : a, index : 0\n",
            "value : b, index : 1\n",
            "value : c, index : 2\n",
            "value : d, index : 3\n",
            "value : e, index : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6-4. 케라스(Keras) 텍스트 전처리"
      ],
      "metadata": {
        "id": "ID6jW2fXuXDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "preprocessed_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1gLgy9SuTQO",
        "outputId": "f3f0c605-aa0c-4f3e-dd04-127af48552ec"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['barber', 'person'],\n",
              " ['barber', 'good', 'person'],\n",
              " ['barber', 'huge', 'person'],\n",
              " ['knew', 'secret'],\n",
              " ['secret', 'kept', 'huge', 'secret'],\n",
              " ['huge', 'secret'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'secret'],\n",
              " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
              " ['barber', 'went', 'huge', 'mountain']]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "\n",
        "#fit_on_texts() 안에 코퍼스(preprocessed_sentence)를 입력으로 하면 빈도수를 기준으로 단어 집합 생성\n",
        "tokenizer.fit_on_texts(preprocessed_sentence) #각 문장별로 총 단어 빈도수가 높은 순으로 낮은 인덱스를 부여한다.\n",
        "print(tokenizer.word_index) #인덱스가 어떻게 부여되었는지 확인한다.\n",
        "print(tokenizer.word_counts) #각 단어가 몇 개 였는지 확인한다.\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentence)) #입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTXkjCv3uruN",
        "outputId": "486f25a4-ea0d-4201-8b42-4a39ff198707"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=5\n",
        "tokenizer=Tokenizer(num_words=vocab_size+1)#상위단어 5개만 사용\n",
        "tokenizer.fit_on_texts(preprocessed_sentence)\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentence)) #위의 두줄은 적용되지 않고 마지막 줄만 적용이 되어있다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0DvclBixEtu",
        "outputId": "ff9a9498-76ed-42f7-c80a-a5c713c56de6"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#한번에 적용을 하는 방법\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentence)\n",
        "vocab_size=5\n",
        "words_frequency=[word for word, index in tokenizer.word_index.items() if index>=vocab_size+1]\n",
        "\n",
        "for word in words_frequency:\n",
        "  del tokenizer.word_index[word]\n",
        "  del tokenizer.word_counts[word]\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQtHHU3cy45I",
        "outputId": "c14b85e3-fb80-4944-a905-08f677af4fb0"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
            "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#keras의 tokenizer은 기본적으로 단어 집합에 없는 OOV는 삭제를 해버린다.\n",
        "#이에 대한 보완으로 oov_token에 OOV를 추가하여 사용할 수 있다.\n",
        "#숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
        "vocab_size=5\n",
        "tokenizer=Tokenizer(num_words=vocab_size+2, oov_token='OOV')\n",
        "tokenizer.fit_on_texts(preprocessed_sentence)\n",
        "print('OOV의 인덱스 : ', tokenizer.word_index['OOV'])\n",
        "#기본적으로 keras tokenizer은 OOV의 인덱스를 1로 설정한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYFz8zLt0XN4",
        "outputId": "4a9f0265-c6f1-41ea-f7ed-e56d001c19a6"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOV의 인덱스 :  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모두 확인이 끝나면 정수 인코딩\n",
        "print(tokenizer.texts_to_sequences(preprocessed_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXK242tL2R_I",
        "outputId": "79dcbe3d-ad1a-476d-be58-49ce6afca8d9"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. 패딩(padding)"
      ],
      "metadata": {
        "id": "6X39tP5u4iDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk.tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm1ytCvvoYh9",
        "outputId": "c34cda32-373f-471d-cee4-a68a583da13b"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement nltk.tokenize (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for nltk.tokenize\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Numpy로 패딩하기\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "ll='Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry\\'s conflict with Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people). The series was originally published in English by Bloomsbury in the United Kingdom and Scholastic Press in the United States. All versions around the world are printed by Grafica Veneta in Italy.[1] A series of many genres, including fantasy, drama, coming-of-age fiction, and the British school story (which includes elements of mystery, thriller, adventure, horror, and romance), the world of Harry Potter explores numerous themes and includes many cultural meanings and references.[2] According to Rowling, the main theme is death.[3] Other major themes in the series include prejudice, corruption, and madness.[4] Since the release of the first novel, Harry Potter and the Philosopher\\'s Stone, on 26 June 1997, the books have found immense popularity, positive reviews, and commercial success worldwide. They have attracted a wide adult audience as well as younger readers and are widely considered cornerstones of modern literature.[5][6] As of February 2023, the books have sold more than 600 million copies worldwide, making them the best-selling book series in history, and have been available in 85 languages.[7] The last four books consecutively set records as the fastest-selling books in history, with the final instalment selling roughly 2.7 million copies in the United Kingdom and 8.3 million copies in the United States within twenty-four hours of its release. The original seven books were adapted into an eight-part namesake film series by Warner Bros. Pictures. In 2016, the total value of the Harry Potter franchise was estimated at $25 billion,[8] making Harry Potter one of the highest-grossing media franchises of all time. Harry Potter and the Cursed Child is a play based on a story co-written by Rowling. The success of the books and films has allowed the Harry Potter franchise to expand with numerous derivative works, a travelling exhibition that premiered in Chicago in 2009, a studio tour in London that opened in 2012, a digital platform on which J. K. Rowling updates the series with new information and insight, and a pentalogy of spin-off films premiering in November 2016 with Fantastic Beasts and Where to Find Them, among many other developments. Themed attractions, collectively known as The Wizarding World of Harry Potter, have been built at several Universal Destinations & Experiences amusement parks around the world.'\n",
        "sentences=sent_tokenize(ll)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxmv5eLr4dib",
        "outputId": "7bdd95ad-dd3b-49c6-cea8-068cd039416b"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling.', 'The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry.', \"The main story arc concerns Harry's conflict with Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people).\", 'The series was originally published in English by Bloomsbury in the United Kingdom and Scholastic Press in the United States.', 'All versions around the world are printed by Grafica Veneta in Italy.', '[1] A series of many genres, including fantasy, drama, coming-of-age fiction, and the British school story (which includes elements of mystery, thriller, adventure, horror, and romance), the world of Harry Potter explores numerous themes and includes many cultural meanings and references.', '[2] According to Rowling, the main theme is death.', '[3] Other major themes in the series include prejudice, corruption, and madness.', \"[4] Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, positive reviews, and commercial success worldwide.\", 'They have attracted a wide adult audience as well as younger readers and are widely considered cornerstones of modern literature.', '[5][6] As of February 2023, the books have sold more than 600 million copies worldwide, making them the best-selling book series in history, and have been available in 85 languages.', '[7] The last four books consecutively set records as the fastest-selling books in history, with the final instalment selling roughly 2.7 million copies in the United Kingdom and 8.3 million copies in the United States within twenty-four hours of its release.', 'The original seven books were adapted into an eight-part namesake film series by Warner Bros. Pictures.', 'In 2016, the total value of the Harry Potter franchise was estimated at $25 billion,[8] making Harry Potter one of the highest-grossing media franchises of all time.', 'Harry Potter and the Cursed Child is a play based on a story co-written by Rowling.', 'The success of the books and films has allowed the Harry Potter franchise to expand with numerous derivative works, a travelling exhibition that premiered in Chicago in 2009, a studio tour in London that opened in 2012, a digital platform on which J. K. Rowling updates the series with new information and insight, and a pentalogy of spin-off films premiering in November 2016 with Fantastic Beasts and Where to Find Them, among many other developments.', 'Themed attractions, collectively known as The Wizarding World of Harry Potter, have been built at several Universal Destinations & Experiences amusement parks around the world.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={}\n",
        "preprocessed_sentences=[]\n",
        "wordstop=set(stopwords.words('english'))\n",
        "for sentence in sentences:\n",
        "  result=[]\n",
        "  tokenized_sentence=word_tokenize(sentence.lower())\n",
        "  for word in tokenized_sentence:\n",
        "    if word not in wordstop:\n",
        "      if len(word)>2:\n",
        "        result.append(word)\n",
        "        if word not in vocab:\n",
        "          vocab[word]=0\n",
        "        vocab[word]+=1\n",
        "  preprocessed_sentences.append(result)\n",
        "print(preprocessed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbLgBZ2f276x",
        "outputId": "7ed24482-3812-42ab-dcb4-e18be18d3308"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['harry', 'potter', 'series', 'seven', 'fantasy', 'novels', 'written', 'british', 'author', 'rowling'], ['novels', 'chronicle', 'lives', 'young', 'wizard', 'harry', 'potter', 'friends', 'hermione', 'granger', 'ron', 'weasley', 'students', 'hogwarts', 'school', 'witchcraft', 'wizardry'], ['main', 'story', 'arc', 'concerns', 'harry', 'conflict', 'lord', 'voldemort', 'dark', 'wizard', 'intends', 'become', 'immortal', 'overthrow', 'wizard', 'governing', 'body', 'known', 'ministry', 'magic', 'subjugate', 'wizards', 'muggles', 'non-magical', 'people'], ['series', 'originally', 'published', 'english', 'bloomsbury', 'united', 'kingdom', 'scholastic', 'press', 'united', 'states'], ['versions', 'around', 'world', 'printed', 'grafica', 'veneta', 'italy'], ['series', 'many', 'genres', 'including', 'fantasy', 'drama', 'coming-of-age', 'fiction', 'british', 'school', 'story', 'includes', 'elements', 'mystery', 'thriller', 'adventure', 'horror', 'romance', 'world', 'harry', 'potter', 'explores', 'numerous', 'themes', 'includes', 'many', 'cultural', 'meanings', 'references'], ['according', 'rowling', 'main', 'theme', 'death'], ['major', 'themes', 'series', 'include', 'prejudice', 'corruption', 'madness'], ['since', 'release', 'first', 'novel', 'harry', 'potter', 'philosopher', 'stone', 'june', '1997', 'books', 'found', 'immense', 'popularity', 'positive', 'reviews', 'commercial', 'success', 'worldwide'], ['attracted', 'wide', 'adult', 'audience', 'well', 'younger', 'readers', 'widely', 'considered', 'cornerstones', 'modern', 'literature'], ['february', '2023', 'books', 'sold', '600', 'million', 'copies', 'worldwide', 'making', 'best-selling', 'book', 'series', 'history', 'available', 'languages'], ['last', 'four', 'books', 'consecutively', 'set', 'records', 'fastest-selling', 'books', 'history', 'final', 'instalment', 'selling', 'roughly', '2.7', 'million', 'copies', 'united', 'kingdom', '8.3', 'million', 'copies', 'united', 'states', 'within', 'twenty-four', 'hours', 'release'], ['original', 'seven', 'books', 'adapted', 'eight-part', 'namesake', 'film', 'series', 'warner', 'bros.', 'pictures'], ['2016', 'total', 'value', 'harry', 'potter', 'franchise', 'estimated', 'billion', 'making', 'harry', 'potter', 'one', 'highest-grossing', 'media', 'franchises', 'time'], ['harry', 'potter', 'cursed', 'child', 'play', 'based', 'story', 'co-written', 'rowling'], ['success', 'books', 'films', 'allowed', 'harry', 'potter', 'franchise', 'expand', 'numerous', 'derivative', 'works', 'travelling', 'exhibition', 'premiered', 'chicago', '2009', 'studio', 'tour', 'london', 'opened', '2012', 'digital', 'platform', 'rowling', 'updates', 'series', 'new', 'information', 'insight', 'pentalogy', 'spin-off', 'films', 'premiering', 'november', '2016', 'fantastic', 'beasts', 'find', 'among', 'many', 'developments'], ['themed', 'attractions', 'collectively', 'known', 'wizarding', 'world', 'harry', 'potter', 'built', 'several', 'universal', 'destinations', 'experiences', 'amusement', 'parks', 'around', 'world']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)\n",
        "print(vocab['harry'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7P_cnLBygqr",
        "outputId": "71a38c31-bce4-420c-92ab-32642b5f671e"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'harry': 10, 'potter': 9, 'series': 7, 'seven': 2, 'fantasy': 2, 'novels': 2, 'written': 1, 'british': 2, 'author': 1, 'rowling': 4, 'chronicle': 1, 'lives': 1, 'young': 1, 'wizard': 3, 'friends': 1, 'hermione': 1, 'granger': 1, 'ron': 1, 'weasley': 1, 'students': 1, 'hogwarts': 1, 'school': 2, 'witchcraft': 1, 'wizardry': 1, 'main': 2, 'story': 3, 'arc': 1, 'concerns': 1, 'conflict': 1, 'lord': 1, 'voldemort': 1, 'dark': 1, 'intends': 1, 'become': 1, 'immortal': 1, 'overthrow': 1, 'governing': 1, 'body': 1, 'known': 2, 'ministry': 1, 'magic': 1, 'subjugate': 1, 'wizards': 1, 'muggles': 1, 'non-magical': 1, 'people': 1, 'originally': 1, 'published': 1, 'english': 1, 'bloomsbury': 1, 'united': 4, 'kingdom': 2, 'scholastic': 1, 'press': 1, 'states': 2, 'versions': 1, 'around': 2, 'world': 4, 'printed': 1, 'grafica': 1, 'veneta': 1, 'italy': 1, 'many': 3, 'genres': 1, 'including': 1, 'drama': 1, 'coming-of-age': 1, 'fiction': 1, 'includes': 2, 'elements': 1, 'mystery': 1, 'thriller': 1, 'adventure': 1, 'horror': 1, 'romance': 1, 'explores': 1, 'numerous': 2, 'themes': 2, 'cultural': 1, 'meanings': 1, 'references': 1, 'according': 1, 'theme': 1, 'death': 1, 'major': 1, 'include': 1, 'prejudice': 1, 'corruption': 1, 'madness': 1, 'since': 1, 'release': 2, 'first': 1, 'novel': 1, 'philosopher': 1, 'stone': 1, 'june': 1, '1997': 1, 'books': 6, 'found': 1, 'immense': 1, 'popularity': 1, 'positive': 1, 'reviews': 1, 'commercial': 1, 'success': 2, 'worldwide': 2, 'attracted': 1, 'wide': 1, 'adult': 1, 'audience': 1, 'well': 1, 'younger': 1, 'readers': 1, 'widely': 1, 'considered': 1, 'cornerstones': 1, 'modern': 1, 'literature': 1, 'february': 1, '2023': 1, 'sold': 1, '600': 1, 'million': 3, 'copies': 3, 'making': 2, 'best-selling': 1, 'book': 1, 'history': 2, 'available': 1, 'languages': 1, 'last': 1, 'four': 1, 'consecutively': 1, 'set': 1, 'records': 1, 'fastest-selling': 1, 'final': 1, 'instalment': 1, 'selling': 1, 'roughly': 1, '2.7': 1, '8.3': 1, 'within': 1, 'twenty-four': 1, 'hours': 1, 'original': 1, 'adapted': 1, 'eight-part': 1, 'namesake': 1, 'film': 1, 'warner': 1, 'bros.': 1, 'pictures': 1, '2016': 2, 'total': 1, 'value': 1, 'franchise': 2, 'estimated': 1, 'billion': 1, 'one': 1, 'highest-grossing': 1, 'media': 1, 'franchises': 1, 'time': 1, 'cursed': 1, 'child': 1, 'play': 1, 'based': 1, 'co-written': 1, 'films': 2, 'allowed': 1, 'expand': 1, 'derivative': 1, 'works': 1, 'travelling': 1, 'exhibition': 1, 'premiered': 1, 'chicago': 1, '2009': 1, 'studio': 1, 'tour': 1, 'london': 1, 'opened': 1, '2012': 1, 'digital': 1, 'platform': 1, 'updates': 1, 'new': 1, 'information': 1, 'insight': 1, 'pentalogy': 1, 'spin-off': 1, 'premiering': 1, 'november': 1, 'fantastic': 1, 'beasts': 1, 'find': 1, 'among': 1, 'developments': 1, 'themed': 1, 'attractions': 1, 'collectively': 1, 'wizarding': 1, 'built': 1, 'several': 1, 'universal': 1, 'destinations': 1, 'experiences': 1, 'amusement': 1, 'parks': 1}\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sorted=sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
        "print(vocab_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCYncvnTBR5w",
        "outputId": "dafc08e1-2632-46cf-d71d-8d9d3156edb3"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('harry', 10), ('potter', 9), ('series', 7), ('books', 6), ('rowling', 4), ('united', 4), ('world', 4), ('wizard', 3), ('story', 3), ('many', 3), ('million', 3), ('copies', 3), ('seven', 2), ('fantasy', 2), ('novels', 2), ('british', 2), ('school', 2), ('main', 2), ('known', 2), ('kingdom', 2), ('states', 2), ('around', 2), ('includes', 2), ('numerous', 2), ('themes', 2), ('release', 2), ('success', 2), ('worldwide', 2), ('making', 2), ('history', 2), ('2016', 2), ('franchise', 2), ('films', 2), ('written', 1), ('author', 1), ('chronicle', 1), ('lives', 1), ('young', 1), ('friends', 1), ('hermione', 1), ('granger', 1), ('ron', 1), ('weasley', 1), ('students', 1), ('hogwarts', 1), ('witchcraft', 1), ('wizardry', 1), ('arc', 1), ('concerns', 1), ('conflict', 1), ('lord', 1), ('voldemort', 1), ('dark', 1), ('intends', 1), ('become', 1), ('immortal', 1), ('overthrow', 1), ('governing', 1), ('body', 1), ('ministry', 1), ('magic', 1), ('subjugate', 1), ('wizards', 1), ('muggles', 1), ('non-magical', 1), ('people', 1), ('originally', 1), ('published', 1), ('english', 1), ('bloomsbury', 1), ('scholastic', 1), ('press', 1), ('versions', 1), ('printed', 1), ('grafica', 1), ('veneta', 1), ('italy', 1), ('genres', 1), ('including', 1), ('drama', 1), ('coming-of-age', 1), ('fiction', 1), ('elements', 1), ('mystery', 1), ('thriller', 1), ('adventure', 1), ('horror', 1), ('romance', 1), ('explores', 1), ('cultural', 1), ('meanings', 1), ('references', 1), ('according', 1), ('theme', 1), ('death', 1), ('major', 1), ('include', 1), ('prejudice', 1), ('corruption', 1), ('madness', 1), ('since', 1), ('first', 1), ('novel', 1), ('philosopher', 1), ('stone', 1), ('june', 1), ('1997', 1), ('found', 1), ('immense', 1), ('popularity', 1), ('positive', 1), ('reviews', 1), ('commercial', 1), ('attracted', 1), ('wide', 1), ('adult', 1), ('audience', 1), ('well', 1), ('younger', 1), ('readers', 1), ('widely', 1), ('considered', 1), ('cornerstones', 1), ('modern', 1), ('literature', 1), ('february', 1), ('2023', 1), ('sold', 1), ('600', 1), ('best-selling', 1), ('book', 1), ('available', 1), ('languages', 1), ('last', 1), ('four', 1), ('consecutively', 1), ('set', 1), ('records', 1), ('fastest-selling', 1), ('final', 1), ('instalment', 1), ('selling', 1), ('roughly', 1), ('2.7', 1), ('8.3', 1), ('within', 1), ('twenty-four', 1), ('hours', 1), ('original', 1), ('adapted', 1), ('eight-part', 1), ('namesake', 1), ('film', 1), ('warner', 1), ('bros.', 1), ('pictures', 1), ('total', 1), ('value', 1), ('estimated', 1), ('billion', 1), ('one', 1), ('highest-grossing', 1), ('media', 1), ('franchises', 1), ('time', 1), ('cursed', 1), ('child', 1), ('play', 1), ('based', 1), ('co-written', 1), ('allowed', 1), ('expand', 1), ('derivative', 1), ('works', 1), ('travelling', 1), ('exhibition', 1), ('premiered', 1), ('chicago', 1), ('2009', 1), ('studio', 1), ('tour', 1), ('london', 1), ('opened', 1), ('2012', 1), ('digital', 1), ('platform', 1), ('updates', 1), ('new', 1), ('information', 1), ('insight', 1), ('pentalogy', 1), ('spin-off', 1), ('premiering', 1), ('november', 1), ('fantastic', 1), ('beasts', 1), ('find', 1), ('among', 1), ('developments', 1), ('themed', 1), ('attractions', 1), ('collectively', 1), ('wizarding', 1), ('built', 1), ('several', 1), ('universal', 1), ('destinations', 1), ('experiences', 1), ('amusement', 1), ('parks', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={}\n",
        "i=1\n",
        "for (word, frequency) in vocab_sorted:\n",
        "  if frequency >1 :\n",
        "    word_to_index[word]=i\n",
        "    i+=1\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgsZDYsYBqLe",
        "outputId": "8bc4b896-6c84-49f9-b614-ac72f503d6f4"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'harry': 1, 'potter': 2, 'series': 3, 'books': 4, 'rowling': 5, 'united': 6, 'world': 7, 'wizard': 8, 'story': 9, 'many': 10, 'million': 11, 'copies': 12, 'seven': 13, 'fantasy': 14, 'novels': 15, 'british': 16, 'school': 17, 'main': 18, 'known': 19, 'kingdom': 20, 'states': 21, 'around': 22, 'includes': 23, 'numerous': 24, 'themes': 25, 'release': 26, 'success': 27, 'worldwide': 28, 'making': 29, 'history': 30, '2016': 31, 'franchise': 32, 'films': 33}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=10\n",
        "word_frequency=[word for word, index in word_to_index.items() if index>vocab_size]\n",
        "for word in word_frequency:\n",
        "  del word_to_index[word]\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZTfFjX6CajP",
        "outputId": "2f47e021-dcaa-412c-9590-149562f8603f"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'harry': 1, 'potter': 2, 'series': 3, 'books': 4, 'rowling': 5, 'united': 6, 'world': 7, 'wizard': 8, 'story': 9, 'many': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index['OOV']=len(word_to_index)+1"
      ],
      "metadata": {
        "id": "r8voNcdaEHq8"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sentences=[]\n",
        "for sentence in preprocessed_sentences:\n",
        "  encoded_sentence=[]\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      encoded_sentence.append(word_to_index[word])\n",
        "    except KeyError:\n",
        "      encoded_sentence.append(word_to_index['OOV'])\n",
        "  encoded_sentences.append(encoded_sentence)\n",
        "print(encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr2ahq5lDQv4",
        "outputId": "669a9a47-9fb5-4151-b475-f0024ffcd449"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 11, 11, 11, 11, 11, 11, 5], [11, 11, 11, 11, 8, 1, 2, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11], [11, 9, 11, 11, 1, 11, 11, 11, 11, 8, 11, 11, 11, 11, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11], [3, 11, 11, 11, 11, 6, 11, 11, 11, 6, 11], [11, 11, 7, 11, 11, 11, 11], [3, 10, 11, 11, 11, 11, 11, 11, 11, 11, 9, 11, 11, 11, 11, 11, 11, 11, 7, 1, 2, 11, 11, 11, 11, 10, 11, 11, 11], [11, 5, 11, 11, 11], [11, 11, 3, 11, 11, 11, 11], [11, 11, 11, 11, 1, 2, 11, 11, 11, 11, 4, 11, 11, 11, 11, 11, 11, 11, 11], [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11], [11, 11, 4, 11, 11, 11, 11, 11, 11, 11, 11, 3, 11, 11, 11], [11, 11, 4, 11, 11, 11, 11, 4, 11, 11, 11, 11, 11, 11, 11, 11, 6, 11, 11, 11, 11, 6, 11, 11, 11, 11, 11], [11, 11, 4, 11, 11, 11, 11, 3, 11, 11, 11], [11, 11, 11, 1, 2, 11, 11, 11, 11, 1, 2, 11, 11, 11, 11, 11], [1, 2, 11, 11, 11, 11, 9, 11, 5], [11, 4, 11, 11, 1, 2, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 5, 11, 3, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 11], [11, 11, 11, 11, 11, 7, 1, 2, 11, 11, 11, 11, 11, 11, 11, 11, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##여기부터 패딩\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "encoded=tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY2LAE2lFAwH",
        "outputId": "065b7c16-79a6-4873-8c81-5fc3747e04ae"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 13, 14, 15, 34, 16, 35, 5], [15, 36, 37, 38, 8, 1, 2, 39, 40, 41, 42, 43, 44, 45, 17, 46, 47], [18, 9, 48, 49, 1, 50, 51, 52, 53, 8, 54, 55, 56, 57, 8, 58, 59, 19, 60, 61, 62, 63, 64, 65, 66], [3, 67, 68, 69, 70, 6, 20, 71, 72, 6, 21], [73, 22, 7, 74, 75, 76, 77], [3, 10, 78, 79, 14, 80, 81, 82, 16, 17, 9, 23, 83, 84, 85, 86, 87, 88, 7, 1, 2, 89, 24, 25, 23, 10, 90, 91, 92], [93, 5, 18, 94, 95], [96, 25, 3, 97, 98, 99, 100], [101, 26, 102, 103, 1, 2, 104, 105, 106, 107, 4, 108, 109, 110, 111, 112, 113, 27, 28], [114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125], [126, 127, 4, 128, 129, 11, 12, 28, 29, 130, 131, 3, 30, 132, 133], [134, 135, 4, 136, 137, 138, 139, 4, 30, 140, 141, 142, 143, 144, 11, 12, 6, 20, 145, 11, 12, 6, 21, 146, 147, 148, 26], [149, 13, 4, 150, 151, 152, 153, 3, 154, 155, 156], [31, 157, 158, 1, 2, 32, 159, 160, 29, 1, 2, 161, 162, 163, 164, 165], [1, 2, 166, 167, 168, 169, 9, 170, 5], [27, 4, 33, 171, 1, 2, 32, 172, 24, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 5, 187, 3, 188, 189, 190, 191, 192, 33, 193, 194, 31, 195, 196, 197, 198, 10, 199], [200, 201, 202, 19, 203, 7, 1, 2, 204, 205, 206, 207, 208, 209, 210, 22, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=max(len(item) for item in encoded)\n",
        "#encoded된 문장 중에 가장 긴 문장의 길이는 41이다\n",
        "print('최대 길이 : ', max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjKSm6_KFfRw",
        "outputId": "0549c7f9-6a6c-4590-8534-1911c715fd84"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최대 길이 :  41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#최대 길이에 맞추어 패딩\n",
        "for sentence in encoded:\n",
        "  while len(sentence) < max_len:\n",
        "    sentence.append(0)\n",
        "padded_up=np.array(encoded)\n",
        "print(padded_up[:10])\n",
        "#숫자 0을 사용한 제로 패딩"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PJw1xLnFsN5",
        "outputId": "e0d29bca-7543-41fa-de02-b8c3b03cf9c0"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1   2   3  13  14  15  34  16  35   5   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [ 15  36  37  38   8   1   2  39  40  41  42  43  44  45  17  46  47   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [ 18   9  48  49   1  50  51  52  53   8  54  55  56  57   8  58  59  19\n",
            "   60  61  62  63  64  65  66   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [  3  67  68  69  70   6  20  71  72   6  21   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [ 73  22   7  74  75  76  77   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [  3  10  78  79  14  80  81  82  16  17   9  23  83  84  85  86  87  88\n",
            "    7   1   2  89  24  25  23  10  90  91  92   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [ 93   5  18  94  95   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [ 96  25   3  97  98  99 100   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [101  26 102 103   1   2 104 105 106 107   4 108 109 110 111 112 113  27\n",
            "   28   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]\n",
            " [114 115 116 117 118 119 120 121 122 123 124 125   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#케라스의 전처리 도구 패딩\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "encoded=tokenizer.texts_to_sequences(preprocessed_sentences)\n"
      ],
      "metadata": {
        "id": "VK6W5fEqGCWQ"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded=pad_sequences(encoded)\n",
        "print(padded[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9zAFfEpG5O5",
        "outputId": "58093032-f924-4f88-dd94-b17505c3db31"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   1   2   3  13  14\n",
            "   15  34  16  35   5]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0  15  36  37  38   8   1   2  39  40  41  42  43\n",
            "   44  45  17  46  47]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  18   9\n",
            "   48  49   1  50  51  52  53   8  54  55  56  57   8  58  59  19  60  61\n",
            "   62  63  64  65  66]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   3  67  68  69  70   6\n",
            "   20  71  72   6  21]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  73  22\n",
            "    7  74  75  76  77]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  10  78  79  14  80\n",
            "   81  82  16  17   9  23  83  84  85  86  87  88   7   1   2  89  24  25\n",
            "   23  10  90  91  92]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   93   5  18  94  95]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  96  25\n",
            "    3  97  98  99 100]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0 101  26 102 103   1   2 104 105 106 107   4 108 109 110\n",
            "  111 112 113  27  28]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0 114 115 116 117 118 119 120\n",
            "  121 122 123 124 125]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded=pad_sequences(encoded, padding='post', maxlen=17)\n",
        "#최대길이를 17로 설정하고 post를 넣으며 뒤에 0이 붙는다.\n",
        "#데이터가 maxlen때문에 손실될 경우 truncationg='post'를 이용하여 뒤에서부터 데이터가 삭제된다.\n",
        "#value=len(tokenizer.word_index)+1을 사용하여 길이보다 1이 큰 수로 0대신 패딩을 할 수 있다.\n",
        "print(padded[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrrGOStaG_9I",
        "outputId": "d81b5e15-3fd7-4127-bf42-c763c60b19ff"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1   2   3  13  14  15  34  16  35   5   0   0   0   0   0   0   0]\n",
            " [ 15  36  37  38   8   1   2  39  40  41  42  43  44  45  17  46  47]\n",
            " [ 53   8  54  55  56  57   8  58  59  19  60  61  62  63  64  65  66]\n",
            " [  3  67  68  69  70   6  20  71  72   6  21   0   0   0   0   0   0]\n",
            " [ 73  22   7  74  75  76  77   0   0   0   0   0   0   0   0   0   0]\n",
            " [ 83  84  85  86  87  88   7   1   2  89  24  25  23  10  90  91  92]\n",
            " [ 93   5  18  94  95   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [ 96  25   3  97  98  99 100   0   0   0   0   0   0   0   0   0   0]\n",
            " [102 103   1   2 104 105 106 107   4 108 109 110 111 112 113  27  28]\n",
            " [114 115 116 117 118 119 120 121 122 123 124 125   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. 원-핫 인코딩\n",
        "단어집합 : 같은 단어의 변환 형태를 포함한 모든 서로 다른 단어의 집합(ex : book과 books를 모두 포함)"
      ],
      "metadata": {
        "id": "kKnjNstgIyzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "tokens=okt.morphs(\"내 이름은 김의인이고, 2달 동안 페르소나에서 일을 하게 되었다.\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFf98yOZIw6v",
        "outputId": "5510bc1d-df3d-42cd-a41c-0fbcb2093c02"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['내', '이름', '은', '김', '의인', '이고', ',', '2', '달', '동안', '페르소나', '에서', '일', '을', '하게', '되었다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={word:index for index, word in enumerate(tokens)}\n",
        "print('단어 집합 : {}'.format(word_to_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZYgHFftHGdQ",
        "outputId": "57d16649-56ac-4b4a-97b0-eccb8d5822c9"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : {'내': 0, '이름': 1, '은': 2, '김': 3, '의인': 4, '이고': 5, ',': 6, '2': 7, '달': 8, '동안': 9, '페르소나': 10, '에서': 11, '일': 12, '을': 13, '하게': 14, '되었다': 15, '.': 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one-hot-vector 함수\n",
        "def one_hot_encoding(word, word_to_index):\n",
        "  one_hot_vector=[0]*(len(word_to_index)) #단어 집합의 개수만큼 0으로 이루어진 행을 만든다\n",
        "  index=word_to_index[word] # 해당 단어의 인덱스 번호를 word_to_index에서 알아낸다\n",
        "  one_hot_vector[index]=1 #해당 단어의 인덱스만 1로 바꾼다\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "_YaMqltYMah6"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in tokens:\n",
        "  print(one_hot_encoding(word, word_to_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTaJeBTkNFRz",
        "outputId": "14b1b623-2a8e-4a21-afb5-fe160a95253a"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#keras를 이용한 one-hot-encoding\n",
        "#fit_on_texts : 빈도수 기준으로 단어 집합 생성\n",
        "#texts_to_sequences : 맵핑하여 정수로 인코딩\n",
        "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
      ],
      "metadata": {
        "id": "6Xfismg4NO-7"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YszATHj3Oazq",
        "outputId": "faf6081d-b418-47b0-b200-14872a9b5480"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded=tokenizer.texts_to_sequences([text])[0] #이중 리스트가 아닌 하나의 리스트로 만들기 위함\n",
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeL0OQOlOrXD",
        "outputId": "06cca7c7-2c43-4780-81b3-ce01711f05a5"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 2, 5, 1, 2, 6, 3, 1, 1, 3, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot=to_categorical(encoded) #모두 원핫인코딩으로 바꿔줌\n",
        "print(one_hot)\n",
        "#원핫벡터의 한계는 모두 1과 0이기 때문에 유사도 검사가 불가능(개와 고양이, 개와 냉장고)하고 공간을 매우 많이 차지한다(단어 집합의 수만큼 차원이 크다)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0C3LkDrPRzj",
        "outputId": "ce23b6d2-1e7d-4d1a-a4b7-779a08345a5d"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.한국어 전처리 패키지"
      ],
      "metadata": {
        "id": "06j2FVpls5P2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
        "#한국어 띄어쓰기 모델"
      ],
      "metadata": {
        "id": "EwvAaLXOP22r"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent='김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'\n",
        "new_sent=sent.replace(' ', '')\n",
        "print(new_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEnOQwWwppjG",
        "outputId": "0116026f-0839-4809-f053-3ff88aab90c7"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "김철수는극중두인격의사나이이광수역을맡았다.철수는한국유일의태권도전승자를가리는결전의날을앞두고10년간함께훈련한사형인유연재(김광수분)를찾으러속세로내려온인물이다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pykospacing import PyKoSpacing\n",
        "# spacing=Spacing()\n",
        "# kospacing_sent=spacing(new_sent)\n",
        "# print(sent)\n",
        "# print(kospacing_sent)"
      ],
      "metadata": {
        "id": "9RhRCnGcqxT8"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git\n",
        "#한국어 맞춤법 검사기 모델, 띄어쓰기 보정"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3lVbvlnq_Us",
        "outputId": "c20f50a9-df4c-46dd-af53-f0622484fbb0"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-78d3cthf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-78d3cthf\n",
            "  Resolved https://github.com/ssut/py-hanspell.git to commit fdc6ca50c19f1c85971437a072d89d4e5ce024b8\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from py-hanspell==1.1) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->py-hanspell==1.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->py-hanspell==1.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->py-hanspell==1.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->py-hanspell==1.1) (3.4)\n",
            "Building wheels for collected packages: py-hanspell\n",
            "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-hanspell: filename=py_hanspell-1.1-py3-none-any.whl size=4810 sha256=dde57bfeb62f1c265abb7bd4621493efea11b50dd8eef956ca43bba91f8f2488\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g98s53gx/wheels/2e/43/cc/753c9e1d91affb9ea40e186cea5654fb9231deb454da6724e5\n",
            "Successfully built py-hanspell\n",
            "Installing collected packages: py-hanspell\n",
            "Successfully installed py-hanspell-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hanspell import spell_checker\n",
        "sent=\"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n",
        "spelled_sent=spell_checker.check(sent)\n",
        "hanspell_sent=spelled_sent.checked\n",
        "print(hanspell_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8du7Y_hFrG48",
        "outputId": "f31c2d0a-2a03-4263-bfa0-53371c553fdb"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soynlp\n",
        "#d=품사 태깅, 단어 토큰화 등을 지원(비지도학습) -> 학습기반"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-pmfcVOsSxH",
        "outputId": "645baab7-11c2-4360-ee82-39f7d2704b9b"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting soynlp\n",
            "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/416.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/416.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.8/416.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.22.4)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#신조어\n",
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "print(okt.morphs('에비시기스 이뒤휘 1월 최애돌 기부 요정'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgIEcc9jsz-9",
        "outputId": "60f11f31-7c9d-4ffb-963b-f3fbca328d14"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['에비시기스', '이', '뒤휘', '1월', '최애', '돌', '기부', '요정']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from soynlp import DoublespaceLineCorpus\n",
        "from soynlp.word import WordExtractor\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgfPzPKIuACh",
        "outputId": "2aaaf063-8b01-4b84-fbcb-19a53cd7c883"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2016-10-20.txt', <http.client.HTTPMessage at 0x7debd8464ca0>)"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#훈련 데이터를 다수 문서로 분리(문장단위)\n",
        "corpus=DoublespaceLineCorpus('2016-10-20.txt')\n",
        "len(corpus) #3만 91개의 문서 존재"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHd-AtvnuOjN",
        "outputId": "9a5146bd-b37b-479d-c058-09fccd7a9de8"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30091"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for document in corpus:\n",
        "  if len(document)>0:\n",
        "    print(document)\n",
        "    i=i+1\n",
        "  if i==3:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC8kceNZuacN",
        "outputId": "ade4de49-80a0-4c59-e595-d8805d72b1ab"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19  1990  52 1 22\n",
            "오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다 독자제공 영상 캡처 연합뉴스  서울 연합뉴스 김은경 기자 사제 총기로 경찰을 살해한 범인 성모 46 씨는 주도면밀했다  경찰에 따르면 성씨는 19일 오후 강북경찰서 인근 부동산 업소 밖에서 부동산업자 이모 67 씨가 나오기를 기다렸다 이씨와는 평소에도 말다툼을 자주 한 것으로 알려졌다  이씨가 나와 걷기 시작하자 성씨는 따라가면서 미리 준비해온 사제 총기를 이씨에게 발사했다 총알이 빗나가면서 이씨는 도망갔다 그 빗나간 총알은 지나가던 행인 71 씨의 배를 스쳤다  성씨는 강북서 인근 치킨집까지 이씨 뒤를 쫓으며 실랑이하다 쓰러뜨린 후 총기와 함께 가져온 망치로 이씨 머리를 때렸다  이 과정에서 오후 6시 20분께 강북구 번동 길 위에서 사람들이 싸우고 있다 총소리가 났다 는 등의 신고가 여러건 들어왔다  5분 후에 성씨의 전자발찌가 훼손됐다는 신고가 보호관찰소 시스템을 통해 들어왔다 성범죄자로 전자발찌를 차고 있던 성씨는 부엌칼로 직접 자신의 발찌를 끊었다  용의자 소지 사제총기 2정 서울 연합뉴스 임헌정 기자 서울 시내에서 폭행 용의자가 현장 조사를 벌이던 경찰관에게 사제총기를 발사해 경찰관이 숨졌다 19일 오후 6시28분 강북구 번동에서 둔기로 맞았다 는 폭행 피해 신고가 접수돼 현장에서 조사하던 강북경찰서 번동파출소 소속 김모 54 경위가 폭행 용의자 성모 45 씨가 쏜 사제총기에 맞고 쓰러진 뒤 병원에 옮겨졌으나 숨졌다 사진은 용의자가 소지한 사제총기  신고를 받고 번동파출소에서 김창호 54 경위 등 경찰들이 오후 6시 29분께 현장으로 출동했다 성씨는 그사이 부동산 앞에 놓아뒀던 가방을 챙겨 오패산 쪽으로 도망간 후였다  김 경위는 오패산 터널 입구 오른쪽의 급경사에서 성씨에게 접근하다가 오후 6시 33분께 풀숲에 숨은 성씨가 허공에 난사한 10여발의 총알 중 일부를 왼쪽 어깨 뒷부분에 맞고 쓰러졌다  김 경위는 구급차가 도착했을 때 이미 의식이 없었고 심폐소생술을 하며 병원으로 옮겨졌으나 총알이 폐를 훼손해 오후 7시 40분께 사망했다  김 경위는 외근용 조끼를 입고 있었으나 총알을 막기에는 역부족이었다  머리에 부상을 입은 이씨도 함께 병원으로 이송됐으나 생명에는 지장이 없는 것으로 알려졌다  성씨는 오패산 터널 밑쪽 숲에서 오후 6시 45분께 잡혔다  총격현장 수색하는 경찰들 서울 연합뉴스 이효석 기자 19일 오후 서울 강북구 오패산 터널 인근에서 경찰들이 폭행 용의자가 사제총기를 발사해 경찰관이 사망한 사건을 조사 하고 있다  총 때문에 쫓던 경관들과 민간인들이 몸을 숨겼는데 인근 신발가게 직원 이모씨가 다가가 성씨를 덮쳤고 이어 현장에 있던 다른 상인들과 경찰이 가세해 체포했다  성씨는 경찰에 붙잡힌 직후 나 자살하려고 한 거다 맞아 죽어도 괜찮다 고 말한 것으로 전해졌다  성씨 자신도 경찰이 발사한 공포탄 1발 실탄 3발 중 실탄 1발을 배에 맞았으나 방탄조끼를 입은 상태여서 부상하지는 않았다  경찰은 인근을 수색해 성씨가 만든 사제총 16정과 칼 7개를 압수했다 실제 폭발할지는 알 수 없는 요구르트병에 무언가를 채워두고 심지를 꽂은 사제 폭탄도 발견됐다  일부는 숲에서 발견됐고 일부는 성씨가 소지한 가방 안에 있었다\n",
            "테헤란 연합뉴스 강훈상 특파원 이용 승객수 기준 세계 최대 공항인 아랍에미리트 두바이국제공항은 19일 현지시간 이 공항을 이륙하는 모든 항공기의 탑승객은 삼성전자의 갤럭시노트7을 휴대하면 안 된다고 밝혔다  두바이국제공항은 여러 항공 관련 기구의 권고에 따라 안전성에 우려가 있는 스마트폰 갤럭시노트7을 휴대하고 비행기를 타면 안 된다 며 탑승 전 검색 중 발견되면 압수할 계획 이라고 발표했다  공항 측은 갤럭시노트7의 배터리가 폭발 우려가 제기된 만큼 이 제품을 갖고 공항 안으로 들어오지 말라고 이용객에 당부했다  이런 조치는 두바이국제공항 뿐 아니라 신공항인 두바이월드센터에도 적용된다  배터리 폭발문제로 회수된 갤럭시노트7 연합뉴스자료사진\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#soynlp는 비지도학습이라 학습과정을 거쳐야 함\n",
        "word_extractor=WordExtractor() #형태소에 해당하는 단어로 분리\n",
        "word_extractor.train(corpus) #훈련, 메모리를 얼마나 썼는지 출력됨\n",
        "word_score=word_extractor.extract() #전체 코퍼스로부터 응집 확률과 브랜칭 엔트로피 단어 점수표를 만든다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqA1XLLOuwc1",
        "outputId": "34cf3644-5ac8-4e32-e446-ebe17a1b6a0b"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training was done. used memory 2.402 Gb\n",
            "all cohesion probabilities was computed. # words = 223348\n",
            "all branching entropies was computed # words = 361598\n",
            "all accessor variety was computed # words = 361598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "응집확률(cohesion)은 한 문자열은 문자 단위로 분리하여 왼쪽부터 순서대로 문자를 추가하며 각 문자열이 주어졌을 때 그 다음 문자가 나올 확률을 계산하여 누적곱을 한 값.\n",
        "응집확률이 높을수록 이 문자열 시퀀스는 하나의 단어로 등장할 확률이 높다"
      ],
      "metadata": {
        "id": "EvNTTJ7SxnOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cohesion(2)=P(반포|반) --> 반포한강공원에 중 '반'이라는 단어가 등장했을 때 '반포'라는 단어로 나올 확률. 숫자가 커질수록 이 확률들을 모두 곱하여 루트곱을 한다"
      ],
      "metadata": {
        "id": "v4uwxftgyKoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#반포한강공원에 라는 문장으로 실습\n",
        "word_score['반포한'].cohesion_forward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNP0QFf1wm3m",
        "outputId": "1412abf3-c9aa-4fc3-d743-6c6c830716fc"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08838002913645132"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_score['반포한강'].cohesion_forward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYsLa3QAy8mX",
        "outputId": "83de4b1c-d6f5-4f16-fc94-46703d8da79f"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19841268168224552"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_score['반포한강공'].cohesion_forward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRg_WsvXztFw",
        "outputId": "5a12a8df-7250-470c-f05e-986228d7091e"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2972877884078849"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_score['반포한강공원'].cohesion_forward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApqMhZeLzw4n",
        "outputId": "fe69e5e4-1156-4d26-b683-95373d7f6905"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.37891487632839754"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_score['반포한강공원에'].cohesion_forward\n",
        "#'에' 까지 붙이니까 응집확률이 떨어진다. 결국 '반포한강공원'이라는 단어가 하나의 단어로 판단하는 가장 적합한 문자열이라는 것을 뜻한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqACpBHh0B6i",
        "outputId": "a97be5ec-5472-4e9c-e5b3-82957a54e79a"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33492963377557666"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "브랜칭 엔트로피(branching entropy) -> 주어진 문자열에서 얼마나 다음 문자가 등장할 수 있는지 판단하는 척도이다.(하나의 완성된 단어에 가까워질수록 단어가 예측이 강하게 되기 때문에 브랜칭 엔트로피는 줄어든다)"
      ],
      "metadata": {
        "id": "NiEEomFP0NoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_score['디스'].right_branching_entropy\n",
        "#디스코, 디스코드, 디스플레이 등 여러 가능성이 존재"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZDDZWGP0E-6",
        "outputId": "b7def691-1273-4e3e-86ce-19951783c3f2"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.6371694761537934"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_score['디스플'].right_branching_entropy\n",
        "#디스플레이라는 단어가 거의 확정적이라 0임"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbs303vR1vPQ",
        "outputId": "0832de51-0278-43ed-ecce-2afdd27487d4"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_score['디스플레이'].right_branching_entropy\n",
        "#조사와 같은 여러 단어가 들어갈 가능성이 높음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1abiYfA2ABA",
        "outputId": "831a28ff-63cd-41c7-f7c8-719aa3c78af2"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.1400392861792916"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from soynlp.tokenizer import LTokenizer\n",
        "scores={word : score.cohesion_forward for word, score in word_score.items()} #cohesion(응집확률)을 기준으로 단어와 점수를 정하고\n",
        "l_tokenizer=LTokenizer(scores=scores)#토큰화\n",
        "l_tokenizer.tokenize(\"국제사회와 우리의 노력들로 범죄를 척결하자\", flatten=False) #flatten=False는 (,)의 L(국제사회), R(와) 형태로 보기 위해서 사용한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAES9VPz2N9I",
        "outputId": "3c3890a3-3370-496d-c4e5-a3fc0c059024"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('국제사회', '와'), ('우리', '의'), ('노력', '들로'), ('범죄', '를'), ('척결', '하자')]"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#띄어쓰기가 되지 않는 문장에서 점수가 높은 글자 별로 찾아낸다.\n",
        "from soynlp.tokenizer import MaxScoreTokenizer\n",
        "maxscore_tokenizer=MaxScoreTokenizer(scores=scores)\n",
        "maxscore_tokenizer.tokenize('국제사회와우리의노력들로범죄를척결하자')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMcw6OUF4Hb5",
        "outputId": "3d0ef700-7327-43c5-92af-38068964739f"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from soynlp.normalizer import *\n",
        "#반복되는 문자 정제 -> 하나로 정규화시켜줌\n",
        "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋ 이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠ', num_repeats=2))\n",
        "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ', num_repeats=2))\n",
        "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅜㅜㅠ', num_repeats=2))\n",
        "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ 이영화존잼쓰ㅠㅠㅠㅜㅠㅠㅠㅠㅠ', num_repeats=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8od8JsbB6nzt",
        "outputId": "5e2c62f7-2fd9-4ad6-ecc2-46e27d014517"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아ㅋㅋ 이영화존잼쓰ㅠㅠ\n",
            "아ㅋㅋ 이영화존잼쓰ㅠㅠ\n",
            "아ㅋㅋ 이영화존잼쓰ㅠㅠㅜㅜㅠ\n",
            "아ㅋㅋ 이영화존잼쓰ㅠㅠㅠㅜㅠㅠ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(repeat_normalize('와하하하하핫', num_repeats=2))\n",
        "print(repeat_normalize('와하하하하하핫', num_repeats=2))\n",
        "print(repeat_normalize('와하하하하하하핫', num_repeats=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BC2itH37FSt",
        "outputId": "5b0a6486-f467-4d43-8e89-970dcd84b116"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "와하하핫\n",
            "와하하핫\n",
            "와하하핫\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install customized_konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiMkfBTY7XFU",
        "outputId": "ac1e538e-c5d5-4307-cd6c-93a1c5ae69dd"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting customized_konlpy\n",
            "  Downloading customized_konlpy-0.0.64-py3-none-any.whl (881 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/881.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/881.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m880.6/881.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.5/881.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jpype1>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from customized_konlpy) (1.4.1)\n",
            "Requirement already satisfied: konlpy>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from customized_konlpy) (0.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from Jpype1>=0.6.1->customized_konlpy) (23.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy>=0.4.4->customized_konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy>=0.4.4->customized_konlpy) (1.22.4)\n",
            "Installing collected packages: customized_konlpy\n",
            "Successfully installed customized_konlpy-0.0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Twitter\n",
        "twitter=Twitter()\n",
        "text='의민이는 사무실로 갔습니다.'\n",
        "print('Twitter 형태소 분석 : ', twitter.morphs(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONi0h2Hi7fzq",
        "outputId": "834f00ef-c408-4179-b045-45b69d5b4e56"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twitter 형태소 분석 :  ['의민', '이', '는', '사무실', '로', '갔습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twitter.add_dictionary('의민이', 'Noun')\n",
        "twitter.morphs(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "VMkMPaOP74Fa",
        "outputId": "5755e5fd-653a-49f0-d0ec-c4bf267be048"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-214-066c3832e717>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'의민이'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Noun'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Okt' object has no attribute 'add_dictionary'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6ZHbVne8v_c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}